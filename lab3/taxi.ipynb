{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "- Santiago Alaniz,  5082647-6, santiago.alaniz@fing.edu.uy\n",
    "- Bruno De Simone,  4914555-0, bruno.de.simone@fing.edu.uy\n",
    "- María Usuca,      4891124-3, maria.usuca@fing.edu.uy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Módulo útil para graficar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calcular_promedios(iteraciones, penalizaciones):\n",
    "    print(f\"Se realizaron {np.mean(iteraciones)} iteraciones, en promedio\")\n",
    "    print(f\"Se recibieron {np.mean(penalizaciones)} penalizaciones, en promedio\")\n",
    "\n",
    "def dibujar_grafico(episodios, datos, subindice, etiqueta_y, titulo, color='blue', escala_log_x=False, escala_log_y=False):\n",
    "    plt.subplot(3, 1, subindice)\n",
    "    plt.plot(episodios, datos, color=color)\n",
    "    \n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel(etiqueta_y)\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    if escala_log_x: plt.xscale('log')\n",
    "    if escala_log_y: plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str.strip())\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacia arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "print()\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos, delay=0.01):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(delay)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        \n",
    "    return iteraciones, penalizaciones, marcos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "\n",
    "for i in range(10):\n",
    "    num_iteraciones, _, _ = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)} iteraciones, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:\n",
    "\n",
    "#### Modelo de la realidad:\n",
    "\n",
    "El modelado de la realidad en la cual se entrena al agente es definido por el framework `gymnasium`, escenario `taxi`. Obtenido de la [documentación](https://gymnasium.farama.org/environments/toy_text/taxi/) además de las posibles acciones se tiene que la ubicación del pasajero y los posibles destinos se representan:\n",
    "\n",
    "**Ubicaciones del pasajero:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "* 4: En taxi\n",
    "\n",
    "**Destinos:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "\n",
    "También de la documentación del framework vemos que define los estados con la siguiente ecuación:\n",
    "```\n",
    "((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "```\n",
    "Se observa que parte de lo que define un estado, y por lo cual se podría considerar que el taxi tiene visión global del escenario en el que se encuentra, es la ubicación del pasajero y su destino. Dado que se conoce el destino la cadena de Markov de la cual se quiere conocer la política óptima, sabemos que la misma es no conexa ya que dado cualquier estado para el cual el destino sea `Rojo` nunca se va a poder pasar a un estado para el cual el destino sea `Verde`.\n",
    "\n",
    "<img src=\"img/diagrama-destinos.png\" align=\"center\"/>\n",
    "\n",
    "Además, al saber la ubicación del pasajero se observa que dado un destino `X` y una ubicación `Y` distinta de 4, que significa que el pasajero está en el taxi, solo se puede pasar a un estado que tenga el mismo destino y ubicación O a uno que tenga el mismo destino y ubicación 4. Además, si se tiene ubicación de pasajero 4 nunca va a poder cambiar la ubicación del pasajero.\n",
    "\n",
    "<img src=\"img/diagrama-ubicacion-pasajero.png\" align=\"center\"/>\n",
    "\n",
    "Se ve entonces que si entrenamos en un escenario cuyo destino es `X` luego para cualquier escenario en el cual el destino sea distinto de `X` el taxista no va a saber qué hacer sin entrenamiento ya que nunca pasó por dicho estado ni va a pasar por ningún estado en el cual el destino sea `X`. Lo mismo ocurre al entrenar para un escenario con ubicación inicial del pasajero `Y`, luego al estar en un escenario con ubicación inicial distinta hasta que no recoja al pasajero siempre va a pasar por estados desconocidos.\n",
    "\n",
    "#### **Clase `AgenteRL`**\n",
    "\n",
    "Esta implementación está fuertemente inspirada en los conceptos presentados en el libro de Mitchell, *Machine Learning* (1997) y las clases impartidas por el equipo docente.\n",
    "\n",
    "El agente interactúa con un entorno provisto por el cuerpo docente, donde debemos mover un taxi en un mapa para recoger y dejar pasajeros. El objetivo es que el taxi aprenda la política óptima, es decir, recoger y dejar al pasajero con la menor cantidad de pasos y con la menor cantidad de penalizaciones.\n",
    "\n",
    "##### **Variables de instancia**:\n",
    "- `self.gamma`: Un factor de descuento, generalmente denotado como gamma (`γ`), que determina cuánto valor le da el agente a las recompensas futuras en comparación con las inmediatas. Toma como valor constante `0.9`. A diferencia de `self.k` y `self.delta_t`, no cambia con el tiempo, aunque somos conscientes de que agentes más vanguardistas aplican técnicas de perfeccionamiento de este valor a lo largo del tiempo.\n",
    "\n",
    "- `self.delta_t`: Es una discretización del tiempo. Se inicializa en 0 y se incrementa en 1 cada vez que el agente toma una acción.\n",
    "\n",
    "- `self.k`: Es el atributo pivotal que re-define la función de distribución `X_s_a` que se utiliza para elegir una acción. `self.k` es la función creciente `f(x) = 1 + log10(delta_t)`, con `delta_t = 0` vale `1` y aumenta en orden logarítmico. Esto último es una decisión de diseño para beneficiar a la exploración en etapas iniciales. Queremos que el agente explore el entorno y no se quede estancado en un mínimo local, por lo que le damos más peso a la exploración en etapas iniciales, y a medida que el agente va aprendiendo, le damos más peso a la explotación.\n",
    "\n",
    "- `self.Q`: Una tabla Q inicializada como una matriz de ceros. Las dimensiones de la matriz dependen del número de estados (`entorno.observation_space.n`) y el número de acciones (`entorno.action_space.n`) en el entorno en el que se encuentra el agente.\n",
    "\n",
    "##### **Métodos**:\n",
    "\n",
    "- `__init__(self, entorno) -> None`: El constructor de la clase que inicializa las variables de instancia.\n",
    "\n",
    "- `elegir_accion(self, estado, max_accion) -> int`: Calcula los nuevos valores del `self.k` y `self.delta_t` y define la función de distribución `X_s_a` para elegir una acción. Luego, elige una acción de acuerdo a la función de distribución `X_s_a` y la devuelve.\n",
    "\n",
    "- `aprender(self, estado_anterior, estado_siguiente, accion, recompensa)`: Este método actualiza la tabla Q utilizando la ecuación de actualización (Bellman). Aquí `Q_max` es el valor máximo de Q para el `estado_siguiente`. La tabla Q para el `estado_anterior` y la `accion` tomada se actualiza considerando la `recompensa` recibida y el valor descontado de `Q_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "        self.delta_t = 0\n",
    "        self.k = 0\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Aumento delta_t y k (tiempo y factor de \"confianza\")\n",
    "        self.delta_t += 1\n",
    "        self.k = 1 + np.log10(self.delta_t)\n",
    "        \n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max_estado_siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar con el muchos episodios con la misma semilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodios = list(range(1, len(num_iteraciones_episodios) + 1))\n",
    "    \n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logarítmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizar los resultados de la ejecución anterior.\n",
    "En nuestro algoritmo, hemos implementado una **estrategia de exploración que evoluciona a lo largo del tiempo**, lo que ha demostrado ser efectivo en el proceso de aprendizaje por refuerzo. Inicialmente, favorecemos la exploración, al ponderar la selección de acciones en función del conocimiento actual del agente y variando el parámetro `k` con el número de iteraciones. Esto permite al agente realizar exploración intensiva al principio y luego cambiar gradualmente hacia la explotación de conocimientos adquiridos. \n",
    "\n",
    "Los vestigios de exploración se pueden ver en los \"picos\" iniciales en la cantidad de iteraciones por episodio, esto da cuenta de la exploración intensiva que realiza el agente al principio. A medida que el agente acumula conocimiento sobre el entorno, la cantidad de iteraciones por episodio disminuye, llegado a un punto en el tiempo donde el agente se vuelve más eficiente en su toma de decisiones y la cantidad de iteraciones por episodio se estabiliza, que además coincide con la convergencia de la política óptima.\n",
    "\n",
    "En nuestros resultados se puede observar que la **cantidad de penalizaciones disminuye** a medida que aumenta el número de episodios. Este comportamiento es esperado, ya que el agente va acumulando conocimiento sobre el entorno y aprende a evitar las acciones que conllevan penalizaciones. Además, el agente se vuelve más eficiente en su toma de decisiones, lo que se refleja en una disminución en la cantidad de iteraciones necesarias para alcanzar su objetivo.\n",
    "\n",
    "En **comparación con el agente aleatorio**, el agente de aprendizaje por refuerzo logra reducir considerablemente la cantidad de iteraciones requeridas para alcanzar sus objetivos. Esto subraya la eficacia del aprendizaje por refuerzo en la mejora del desempeño del agente y su capacidad para tomar decisiones más informadas a lo largo del tiempo.\n",
    "\n",
    "Para poder apreciar mejor la evolución del agente, se puede ejecutar el siguiente código que muestra una serie de episodios de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.unique(num_iteraciones_episodios)[:10][::-1]\n",
    "\n",
    "index_episodios = [num_iteraciones_episodios.index(x) for x in arr]\n",
    "\n",
    "for i in index_episodios:\n",
    "  delay = max(0.01, 6/len(marcos_episodios[i]))\n",
    "  print_frames(marcos_episodios[i], delay=delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se mantiene el rendimiento si cambiamos la semilla? ¿Por qué?\n",
    "\n",
    "Como se vio anteriormente el ambiente en el cual nuestro agente realiza sus tareas, formalmente, es un **DMDP, Deterministic Markov Decision Process**. Esto significa que dado un estado y una acción, el ambiente siempre responde de la misma manera. Esto se modela fácilmente en el código de la sección anterior.\n",
    "\n",
    "Sin embargo, si en cada una de las 1000 iteraciones se cambia la semilla, el taxi se va a encontrar con estados en los cuales nunca estuvo durante su entrenamiento previo. Entonces para cualquiera de las posibles acciones a tomar en esos caso el taxista va a pensar que la recompensa es la misma, por lo cual se va a comportar de manera aleatoria hasta que entrene lo suficiente en escenarios similares. Dado lo visto previamente el taxista deberá entrenarse en todos los subconjuntos de cadenas de markov que tiene el modelo del problema.\n",
    "\n",
    "Tomando en cuenta lo anterior, tiene sentido que el desempeño del agente sea negativo. En un escenario con destino distinto, el agente no puede lograr el mismo rendimiento, ya que los estados en los cuales entreno son disjuntos con los estados por los cuales va a pasar en el escenario. Por otro lado, para escenarios con igual destino y ubicación de pasajero que en el cual entrenó, el rendimiento del taxista debería ser bueno. Se va a experimentar con esta idea a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semillas con mismo Destino y Ubicación inicial de pasajero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla_primera = 1\n",
    "semilla_segunda = 9\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "entorno.reset(seed = semilla_primera)\n",
    "print(f'Entorno con primera semilla {semilla_primera}')\n",
    "print_env(entorno)\n",
    "\n",
    "\n",
    "entorno.reset(seed = semilla_segunda)\n",
    "print(f'Entorno con segunda semilla {semilla_segunda}')\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ejecutar_episodio(agente, semilla_primera)\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla_segunda)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "fig.suptitle('Semillas con mismo Destino y Ubicación inicial de pasajero.', fontsize=16)\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logarítmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semillas con mismo Destino pero distinta Ubicación inicial de pasajero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla_primera = 1\n",
    "semilla_segunda = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "entorno.reset(seed = semilla_primera)\n",
    "print(f'Entorno con primera semilla {semilla_primera}')\n",
    "print_env(entorno)\n",
    "\n",
    "\n",
    "entorno.reset(seed = semilla_segunda)\n",
    "print(f'Entorno con segunda semilla {semilla_segunda}')\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ejecutar_episodio(agente, semilla_primera)\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla_segunda)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "fig.suptitle('Semillas con mismo Destino pero distinta Ubicación inicial de pasajero.', fontsize=16)\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logarítmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semillas con distinto Destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla_primera = 1\n",
    "semilla_segunda = 3\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "entorno.reset(seed = semilla_primera)\n",
    "print(f'Entorno con primera semilla {semilla_primera}')\n",
    "print_env(entorno)\n",
    "\n",
    "\n",
    "entorno.reset(seed = semilla_segunda)\n",
    "print(f'Entorno con segunda semilla {semilla_segunda}')\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ejecutar_episodio(agente, semilla_primera)\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla_segunda)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "fig.suptitle('Semillas con distinto Destino.', fontsize=16)\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logarítmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos como claramente lo asumido en el pre-análisis ocurre: \n",
    "- En el caso con mismo origen de pasajero y destino el taxista tiene un rendimiento excelente. \n",
    "- Por otro lado, empeora al tener un origen de pasajero distinto ya que hasta que recoge al pasajero el taxista únicamente transita estados no descubiertos y sobre el final del trayecto transita estados conocidos. Cómo conoce el destino, no hay penalizaciones ya que el taxista sabe donde dejar al pasajero.\n",
    "- Finalmente, en el escenario con destino distinto al entrenado, se comporta como un agente sin entrenamiento al inicio y aprende con ritmo similar que con el escenario de la primera semilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANTES (Vimos que no es estocástico el modelo) Se mantiene el rendimiento si cambiamos la semilla? ¿Por qué?\n",
    "\n",
    "\n",
    "En el código anterior, el ambiente en el cual nuestro agente realiza sus tareas, formalmente, es un **DMDP, Deterministic Markov Decision Process**. Esto significa que dado un estado y una acción, el ambiente siempre responde de la misma manera. Esto se modela fácilmente en el código de la sección anterior al mantener invariante la semilla.\n",
    "\n",
    "Ahora, si en cada una de las 1000 iteraciones, cambiamos la semilla, el ambiente se vuelve **estocástico/no determinista**, es decir, dado un estado y una acción, el ambiente puede responder de diferentes maneras, dado que la composición del entorno cambia.\n",
    "\n",
    "En particular, en este ambiente, **la posición del taxi y las paradas de origen/destino son aleatorias** (aunque no su posición en la grilla); todo lo demás se mantiene constante. Es correcto afirmar que la función de recompensa es determinista para las acciones de moverse, pero no para las acciones de recoger/dejar pasajeros.\n",
    "\n",
    "Tomando en cuenta lo anterior, tiene sentido que el desempeño del agente sea negativo. En un ambiente estocástico, el agente no puede lograr el mismo rendimiento, ya que no puede confiar en que el ambiente se comporte de la misma manera en cada iteración, y lo que es más importante, a través del tiempo (que es como se configura el atributo `self.k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui\n",
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, i)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logarítmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos mejorar el agente para que se desempeñe bien usando cualquier semilla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL_v2(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "        self.delta_t = 0\n",
    "        self.k = 0\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "        self.visits = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Aumento delta_t y k (tiempo y factor de \"confianza\")\n",
    "        self.delta_t += 1\n",
    "        self.k = 1 + np.log10((self.delta_t))\n",
    "        \n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        deterministic_update = recompensa + self.gamma * Q_max_estado_siguiente\n",
    "\n",
    "        self.visits[estado_anterior, accion] += 1\n",
    "\n",
    "        alpha = 1 / (1 + self.visits[estado_anterior, accion])\n",
    "        \n",
    "        self.Q[estado_anterior, accion] = (1 - alpha) * self.Q[estado_anterior, accion] + alpha * (deterministic_update)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui\n",
    "agente_deterministico = AgenteRL(entorno)\n",
    "agente_estocastico = AgenteRL_v2(entorno)\n",
    "\n",
    "BASE_SEED = 50826476\n",
    "\n",
    "num_iter_agentes = [[], []]\n",
    "num_pen_agentes = [[], []]\n",
    "\n",
    "for i in range(1000):\n",
    "    iter_agent, pen_agent, _ = ejecutar_episodio(agente_deterministico, i+BASE_SEED)\n",
    "    num_iter_agentes[0] += [iter_agent]\n",
    "    num_pen_agentes[0] += [pen_agent]\n",
    "\n",
    "    iter_agent, pen_agent, _ = ejecutar_episodio(agente_estocastico, i+BASE_SEED)\n",
    "    num_iter_agentes[1] += [iter_agent]\n",
    "    num_pen_agentes[1] += [pen_agent]\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_grafico(\n",
    "    range(1, len(num_iter_agentes[0]) + 1),\n",
    "    num_iter_agentes[0], \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(\n",
    "    range(1, len(num_iter_agentes[1]) + 1),\n",
    "    num_iter_agentes[1], \n",
    "    2, \n",
    "    'ESTOCASTICO Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica x,y)',\n",
    "    color='red', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "np_iter_agent_deterministico = np.array(num_iter_agentes[0])[-500:]\n",
    "np_iter_agent_estocastico = np.array(num_iter_agentes[1])[-500:]\n",
    "\n",
    "print(f\"[Deterministico]\\n \\\n",
    "            media: {np.mean(np_iter_agent_deterministico)},\\\n",
    "            std: {np.std(np_iter_agent_deterministico)}\")\n",
    "\n",
    "print(f\"[Estocastico]\\n \\\n",
    "            media: {np.mean(np_iter_agent_estocastico)},\\\n",
    "            std: {np.std(np_iter_agent_estocastico)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
