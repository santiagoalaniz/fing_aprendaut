{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "- Santiago Alaniz,  5082647-6, santiago.alaniz@fing.edu.uy\n",
    "- Bruno De Simone,  4914555-0, bruno.de.simone@fing.edu.uy\n",
    "- María Usuca,      4891124-3, maria.usuca@fing.edu.uy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de Acciones Discrete(6)\n",
      "Espacio de Estados Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacie arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución, debido a que la semilla_acciones es diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(\"Iteraciones: {iteraciones}\")\n",
    "    print(\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(.01)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "    return iteraciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(10):\n",
    "    num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)}, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class AgenteRL(Agente):\n",
    "    # Agregar código aqui\n",
    "\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        # Agregar código aqui\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Agregar código aqui\n",
    "        return 0\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Agregar código aqui\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios con la misma semilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "for i in range(1000):\n",
    "    num_iteraciones = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los resultados aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantiene el rendimiento si cambiamos la semilla? Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mejorar el agente para que se desempeñe bien usando cualquier semilla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregar código aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
