{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "- Santiago Alaniz,  5082647-6, santiago.alaniz@fing.edu.uy\n",
    "- Bruno De Simone,  4914555-0, bruno.de.simone@fing.edu.uy\n",
    "- María Usuca,      4891124-3, maria.usuca@fing.edu.uy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo util para graficar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calcular_promedios(iteraciones, penalizaciones):\n",
    "    print(f\"Se realizaron {np.mean(iteraciones)} iteraciones, en promedio\")\n",
    "    print(f\"Se recibieron {np.mean(penalizaciones)} penalizaciones, en promedio\")\n",
    "\n",
    "def dibujar_grafico(episodios, datos, subindice, etiqueta_y, titulo, color='blue', escala_log_x=False, escala_log_y=False):\n",
    "    plt.subplot(3, 1, subindice)\n",
    "    plt.plot(episodios, datos, color=color)\n",
    "    \n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel(etiqueta_y)\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    if escala_log_x: plt.xscale('log')\n",
    "    if escala_log_y: plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacia arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos, delay=0.01):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(delay)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        \n",
    "    return iteraciones, penalizaciones, marcos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "\n",
    "for i in range(10):\n",
    "    num_iteraciones, _, _ = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)} iteraciones, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea a realizar consiste en programar un agente de aprendizaje por refuerzos:\n",
    "\n",
    "\n",
    "#### **Clase `AgenteRL`**\n",
    "\n",
    "La clase `AgenteRL` es una implementación de un agente de aprendizaje por refuerzo. Esta implementación esta fuertemente inspirada en los conceptos presentados en el libro de Mitchell, *Machine Learning* (1997) y las clases impartidas por el equipo docente. \n",
    "\n",
    "El agente interactúa con un entorno provisto por la biblioteca `Gymnassium`. El entorno es un problema de aprendizaje por refuerzo clásico llamado `*Taxi-v3*`. Donde debemos mover un taxi en un mapa para recoger y dejar pasajeros. El objetivo es que el taxi aprenda a recoger y dejar pasajeros en el menor tiempo posible, es decir, con la menor cantidad de pasos y con la menor cantidad de penalizaciones.\n",
    "\n",
    "##### **Variables de instancia**:\n",
    "- `self.gamma`: Un factor de descuento, generalmente denotado como gamma (`γ`), que determina cuánto valor le da el agente a las recompensas futuras en comparación con las inmediatas. Toma como valor constante e inicial 0.9. A diferencia de `self.k` y `self.delta_t`, no cambia con el tiempo, aunque somos conscientes que agentes mas vanguardistas aplican tecnicas de perfeccionamiento de este valor a lo largo del tiempo.\n",
    "\n",
    "\n",
    "- `self.delta_t`: Es una discretización del tiempo. Se inicializa en 0 y se incrementa en 1 cada vez que el agente toma una acción, es decir, tomar una acción es un evento que ocurre en un tiempo `delta_t` y luego el tiempo avanza a `delta_t + 1`.\n",
    "\n",
    "- `self.k`: Es el atributo pivotal de la funcion de distribucion `X_s_a` que se utiliza para elegir una accion. `self.k` es la funcion creciente `f`, que toma a `self.delta_t`\n",
    "como argumento, con `delta_t = 0` vale `1` y aumenta en orden logaritmico. Esto ultimo es una decision de diseño beneficiar a la exploracion en etapas iniciales. A medida que `self.k` crece en el tiempo, la exploracion se vuelve menos probable y la explotacion mas probable, por eso, al tomar un crecimiento logartimico, ponderamos mas la exploracion que la explotacion.\n",
    "\n",
    "- `self.Q`: Una tabla Q inicializada como una matriz de ceros. Las dimensiones de la matriz dependen del número de estados (`entorno.observation_space.n`) y el número de acciones (`entorno.action_space.n`) en el entorno en el que se encuentra el agente.\n",
    "  \n",
    "\n",
    "##### **Métodos**:\n",
    "\n",
    "- `__init__(self, entorno) -> None`: El constructor de la clase que inicializa las variables de instancia. \n",
    "\n",
    "- `elegir_accion(self, estado, max_accion) -> int`: Calcula los nuevos valores del `self.k` y `self.delta_t` y define la funcion de distribucion `X_s_a` para elegir una accion. Luego, elige una accion de acuerdo a la funcion de distribucion `X_s_a` y la devuelve.\n",
    "\n",
    "- `aprender(self, estado_anterior, estado_siguiente, accion, recompensa)`: Este método actualiza la tabla Q utilizando la ecuación de actualización de Q-learning (Bellman). Aquí `Q_max` es el valor máximo de Q para el `estado_siguiente`. La tabla Q para el `estado_anterior` y la `accion` tomada se actualiza considerando la `recompensa` recibida y el valor descontado de `Q_max`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "        self.delta_t = 0\n",
    "        self.k = 0\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Aumento delta_t y k (tiempo y factor de \"confianza\")\n",
    "        self.delta_t += 1\n",
    "        self.k = 1 + np.log10(self.delta_t)\n",
    "        \n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max = np.max(self.Q[estado_siguiente])\n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ejecutar con el muchos episodios con la misma semilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advertencia: este bloque es un loop infinito si el agente se deja sin implementar\n",
    "\n",
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "episodios = list(range(1, len(num_iteraciones_episodios) + 1))\n",
    "    \n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_grafico(\n",
    "    episodios, \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logaritmica x,y)', \n",
    "    escala_log_x=True, \n",
    "    escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_grafico(episodios,\n",
    " num_penalizaciones_episodios,\n",
    " 2,\n",
    " 'Penalizaciones',\n",
    " 'Penalizaciones por episodio (escala logaritmica x)',\n",
    " escala_log_x=True,\n",
    " color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIME = 5\n",
    "marcos_episodicos_de_interes = [10]\n",
    "\n",
    "for i in marcos_episodicos_de_interes:\n",
    "    print_frames(marcos_episodios[i], delay= TOTAL_TIME / len(marcos_episodios[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar los resultados de la ejecución anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro algoritmo, hemos implementado una **estrategia de exploración que evoluciona a lo largo del tiempo**, lo que ha demostrado ser efectivo en el proceso de aprendizaje por refuerzo. Inicialmente, favorecemos la exploración, al ponderar la selección de acciones en función del conocimiento actual del agente y variando el parámetro `k` con el número de iteraciones. Esto permite al agente realizar exploración intensiva al principio y luego cambiar gradualmente hacia la explotación de conocimientos adquiridos.\n",
    "\n",
    "En nuestros resultados se puede observar que la **cantidad de penalizaciones disminuye** a medida que aumenta el número de episodios. Este comportamiento era esperado, ya que el agente va acumulando conocimiento sobre el entorno y aprende a evitar las acciones que conllevan penalizaciones. Además, el agente se vuelve más eficiente en su toma de decisiones, lo que se refleja en una disminución en la cantidad de iteraciones necesarias para alcanzar su objetivo.\n",
    "\n",
    "En **comparación con el agente aleatorio**, el agente de aprendizaje por refuerzo logra reducir considerablemente la cantidad de iteraciones requeridas para alcanzar sus objetivos. Esto subraya la eficacia del aprendizaje por refuerzo en la mejora del desempeño del agente y su capacidad para tomar decisiones más informadas a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantiene el rendimiento si cambiamos la semilla? Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui\n",
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, i)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "calcular_promedios(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "dibujar_grafico(episodios, num_iteraciones_episodios, 1, 'Iteraciones', 'Iteraciones por episodio', escala_log_x=True, escala_log_y=True)\n",
    "dibujar_grafico(episodios, num_penalizaciones_episodios, 2, 'Penalizaciones', 'Penalizaciones por episodio', color='red', escala_log_x=True)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mejorar el agente para que se desempeñe bien usando cualquier semilla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar código aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
