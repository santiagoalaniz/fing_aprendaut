{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial del Framework Gymnasium y Herramientas de Trabajo proporcionadas por la cátedra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str.strip())\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacia arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "print()\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos, delay=0.01):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(delay)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        \n",
    "    return iteraciones, penalizaciones, marcos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "\n",
    "for i in range(10):\n",
    "    num_iteraciones, _, _ = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)} iteraciones, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "| Nombre           | C.I     | Email                        |\n",
    "|----------------|-----------|------------------------------|\n",
    "| Santiago Alaniz| 5082647-6 | santiago.alaniz@fing.edu.uy  |\n",
    "| Bruno De Simone| 4914555-0 | bruno.de.simone@fing.edu.uy  |\n",
    "| María Usuca    | 4891124-3 | maria.usuca@fing.edu.uy      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduccion\n",
    "\n",
    "\n",
    "El modelado de la realidad en la cual se entrena al agente es definido por el framework `gymnasium`, escenario `taxi`. Obtenido de la [documentación](https://gymnasium.farama.org/environments/toy_text/taxi/) además de las posibles acciones se tiene que la ubicación del pasajero y los posibles destinos se representan:\n",
    "\n",
    "**Ubicaciones del pasajero:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "* 4: En taxi\n",
    "\n",
    "**Destinos:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "\n",
    "También de la documentación del framework vemos que define los estados con la siguiente ecuación:\n",
    "\n",
    "```\n",
    "((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "```\n",
    "\n",
    "Se observa que parte de lo que define un estado, y por lo cual se podría considerar que el taxi tiene visión global del escenario en el que se encuentra, es la ubicación del pasajero y su destino. Dado que se conoce el destino la cadena de Markov de la cual se quiere conocer la política óptima, sabemos que la misma es no conexa ya que dado cualquier estado para el cual el destino sea `Rojo` nunca se va a poder pasar a un estado para el cual el destino sea `Verde`.\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-destinos.png\"/>\n",
    "  </p>\n",
    "<br>\n",
    "\n",
    "Además, al saber la ubicación del pasajero se observa que dado un destino `X` y una ubicación `Y` distinta de 4, que significa que el pasajero está en el taxi, solo se puede pasar a un estado que tenga el mismo destino y ubicación O a uno que tenga el mismo destino y ubicación 4. Además, si se tiene ubicación de pasajero 4 nunca va a poder cambiar la ubicación del pasajero.\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-ubicacion-pasajero.png\"/>\n",
    "  </p>\n",
    "<br>\n",
    "\n",
    "Se ve entonces que si entrenamos en un escenario cuyo destino es `X` luego para cualquier escenario en el cual el destino sea distinto de `X` el taxista no va a saber qué hacer sin entrenamiento ya que nunca pasó por dicho estado ni va a pasar por ningún estado en el cual el destino sea `X`. Lo mismo ocurre al entrenar para un escenario con ubicación inicial del pasajero `Y`, luego al estar en un escenario con ubicación inicial distinta hasta que no recoja al pasajero siempre va a pasar por estados desconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "Programar las funciones de la clase `AgenteRL`, manteniendo cualquier función adicional necesaria en la misma clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clase `AgenteRL`**\n",
    "\n",
    "Esta implementación está fuertemente inspirada en los conceptos presentados en el libro de Mitchell, *Machine Learning* (1997) y las clases impartidas por el equipo docente.\n",
    "\n",
    "El agente interactúa con un entorno provisto por el cuerpo docente, donde debemos mover un taxi en un mapa para recoger y dejar pasajeros. El objetivo es que el taxi aprenda la política óptima, es decir, recoger y dejar al pasajero con la menor cantidad de pasos y con la menor cantidad de penalizaciones.\n",
    "\n",
    "##### **Variables de instancia**:\n",
    "- `self.gamma`: Un factor de descuento, generalmente denotado como gamma (`γ`), que determina cuánto valor le da el agente a las recompensas futuras en comparación con las inmediatas. Toma como valor constante `0.9`. A diferencia de `self.k` y `self.delta_t`, no cambia con el tiempo, aunque somos conscientes de que agentes más vanguardistas aplican técnicas de perfeccionamiento de este valor a lo largo del tiempo.\n",
    "\n",
    "- `self.delta_t`: Es una discretización de las acciones tomadas en un episodio. Se inicializa en 0 y se incrementa en 1 cada vez que el agente toma una acción.\n",
    "\n",
    "- `self.k`: Es el atributo pivotal que re-define la función de distribución `X_s_a` que se utiliza para elegir una acción. `self.k` es la función creciente `f(x) = 1 + log10(delta_t)`, con `delta_t = 0` vale `1` y aumenta en orden logarítmico. Esto último es una decisión de diseño para beneficiar a la exploración en etapas iniciales. Queremos que el agente explore el entorno y no se quede estancado en un mínimo local, por lo que le damos más peso a la exploración en etapas iniciales, y a medida que el agente va aprendiendo, le damos más peso a la explotación.\n",
    "\n",
    "- `self.Q`: Una tabla Q inicializada como una matriz de ceros. Las dimensiones de la matriz dependen del número de estados (`entorno.observation_space.n`) y el número de acciones (`entorno.action_space.n`) en el entorno en el que se encuentra el agente.\n",
    "\n",
    "##### **Métodos**:\n",
    "\n",
    "- `__init__(self, entorno) -> None`: El constructor de la clase que inicializa las variables de instancia.\n",
    "\n",
    "- `elegir_accion(self, estado, max_accion) -> int`: Define la función de distribución `X_s_a` para el valor de `self.K` actual, tal cual presentada en el libro del curso. Luego, utilizando el metodo `np.random.choice` elige una acción de acuerdo a la función de distribución `X_s_a` y la devuelve.\n",
    "\n",
    "- `aprender(self, estado_anterior, estado_siguiente, accion, recompensa)`: Este método actualiza la tabla Q utilizando la ecuación de actualización (Bellman). Aquí `Q_max` es el valor máximo de Q para el `estado_siguiente`. La tabla Q para el `estado_anterior` y la `accion` tomada se actualiza considerando la `recompensa` recibida y el valor descontado de `Q_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = 0.9\n",
    "        self.k = 1\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "    \n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Aumento k (factor de \"confianza\")\n",
    "        self.k += 1\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max_estado_siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "\n",
    "def metricas(iteraciones, penalizaciones):\n",
    "  print(f\"[ITERACIONES]       mean: {np.mean(iteraciones)}, std: {np.std(iteraciones)}\")\n",
    "  print(f\"[PENALIZACIONES]    mean: {np.mean(penalizaciones)}, std: {np.std(penalizaciones)}\")\n",
    "  \n",
    "def dibujar_subgrafico(episodios, datos, subindice, etiqueta_y, titulo, color='blue', escala_log_x=False, escala_log_y=False):\n",
    "    plt.subplot(3, 1, subindice)\n",
    "    plt.plot(episodios, datos, color=color)\n",
    "    \n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel(etiqueta_y)\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    if escala_log_x: plt.xscale('log')\n",
    "    if escala_log_y: plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "Analizar los resultados de una ejecución de mil episodios con el agente programado. Agregar un nuevo bloque de de texto discutiendo los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de resultados\n",
    "\n",
    "En nuestro algoritmo, hemos implementado una **estrategia de exploración que evoluciona a lo largo del tiempo**, lo que ha demostrado ser efectivo en el proceso de aprendizaje por refuerzo. Inicialmente, favorecemos la exploración, al ponderar la selección de acciones en función del conocimiento actual del agente y variando el parámetro `k` con el número de iteraciones. Esto permite al agente realizar exploración intensiva al principio y luego cambiar gradualmente hacia la explotación de conocimientos adquiridos. \n",
    "\n",
    "Los vestigios de exploración se pueden ver en los \"picos\" iniciales en la cantidad de iteraciones por episodio, esto da cuenta de la exploración intensiva que realiza el agente al principio. A medida que el agente acumula conocimiento sobre el entorno, la cantidad de iteraciones por episodio disminuye, llegado a un punto en el tiempo donde el agente se vuelve más eficiente en su toma de decisiones y la cantidad de iteraciones por episodio se estabiliza, que además coincide con la convergencia de la política óptima.\n",
    "\n",
    "En nuestros resultados se puede observar que la **cantidad de penalizaciones disminuye** a medida que aumenta el número de episodios. Este comportamiento es esperado, ya que el agente va acumulando conocimiento sobre el entorno y aprende a evitar las acciones que conllevan penalizaciones. Además, el agente se vuelve más eficiente en su toma de decisiones, lo que se refleja en una disminución en la cantidad de iteraciones necesarias para alcanzar su objetivo.\n",
    "\n",
    "En **comparación con el agente aleatorio**, el agente de aprendizaje por refuerzo logra reducir considerablemente la cantidad de iteraciones requeridas para alcanzar sus objetivos. Esto subraya la eficacia del aprendizaje por refuerzo en la mejora del desempeño del agente y su capacidad para tomar decisiones más informadas a lo largo del tiempo.\n",
    "\n",
    "Para poder apreciar mejor la evolución del agente, se puede ejecutar el siguiente código que muestra una serie de episodios de interés.\n",
    "\n",
    "*Nota*:\n",
    "\n",
    "Dependiendo de la semilla utilizada en la clase, se pueden apreciar pequeñas variaciones post convergencia. Esto se debe a que a cada accion se le asigna una probabilidad no nula, y aun siendo muy pequeña, existe la posibilidad de que el agente tome una accion aleatoria en lugar de la accion optima. Esto se corrige cambiando la funcion que define `self.k` a `f(x) = 1 + sqrt(delta_t)`, es decir aumentando el orden de crecimiento de la funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "  num_iteraciones_episodios, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "  escala_log_x=True, \n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "  num_penalizaciones_episodios,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio (escala logarítmica x)',\n",
    "  escala_log_x=True,\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3\n",
    "\n",
    "Ejecutar 1000 episodios con una semilla diferente y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos.\n",
    "\n",
    "### Metodologia\n",
    "\n",
    "En el código anterior, el ambiente en el cual nuestro agente realiza sus tareas, formalmente, es un **DMDP, Deterministic Markov Decision Process**. Esto significa que dado un estado y una acción, el ambiente siempre responde de la misma manera. Esto se modela fácilmente en el código de la sección anterior al mantener invariante la semilla.\n",
    "\n",
    "Ahora, si cambiamos la semilla, el ambiente puede responder de diferentes maneras, dado que la composición del entorno cambia.\n",
    "\n",
    "En particular, **la posición del taxi y las paradas de origen/destino son aleatorias** (aunque no su posición en la grilla); todo lo demás se mantiene constante.\n",
    "\n",
    "Proponemos entonces, para analizar el comportamiento del agente variando la semilla, tomando la semilla inicial definir el par (semilla inicial (SI), semilla alternativa (SA)) de forma tal que:\n",
    "\n",
    "- (SI, SA) difieren solo en la posición del taxi.\n",
    "- (SI, SA) difieren en la posición del taxi y en la posición del origen.\n",
    "- (SI, SA) difieren en la posición del taxi y en la posición del destino.\n",
    "\n",
    "Tomando la siguiente proporción de ejecuciones:\n",
    "\n",
    "- 60% con la misma semilla (SI). Para cada una de las variantes\n",
    "  - 40% con una semilla alternativa (SA) que difiera solo en la posición del taxi.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en la posición del taxi y en la posición del destino.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en la posición del taxi, en la posición del origen y en la posición del destino. Es decir, en todo.\n",
    "\n",
    "De esta forma podemos llevar el agente de un entorno conocido a uno ligéramente diferente, y luego a uno completamente diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metodologia = {\n",
    "  'taxi': (1, 9),\n",
    "  'taxi_destino': (1, 18),\n",
    "  'taxi_origen_destino': (1, 7),\n",
    "}\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "for metodo in metodologia.items():\n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  for seed in metodo[1]:\n",
    "    entorno.reset(seed = seed)\n",
    "    print(f'SEED {seed}')\n",
    "    print_env(entorno)\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metodo in metodologia.items():\n",
    "  agente = AgenteRL(entorno)\n",
    "  num_iteraciones_episodios = []\n",
    "  num_penalizaciones_episodios = []\n",
    "  \n",
    "  for i in range(600):\n",
    "    entorno.reset(seed = metodo[1][0])\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente, metodo[1][0])\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "  \n",
    "  for i in range(400):\n",
    "    entorno.reset(seed = metodo[1][1])\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente, metodo[1][1])\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "  \n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "  plt.figure(figsize=(15,10))\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica y)',\n",
    "    escala_log_y=True\n",
    "  )\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "    num_penalizaciones_episodios,\n",
    "    2,\n",
    "    'Penalizaciones',\n",
    "    'Penalizaciones por episodio',\n",
    "    color='red',\n",
    "  )\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de resultados\n",
    "\n",
    "Hay una correlación directa entre la cantidad de iteraciones por episodio y la cantidad de penalizaciones. Como sospechabamos, el agente se comporta de manera diferente en cada entorno, y esto se ve reflejado en la cantidad de iteraciones por episodio y la cantidad de penalizaciones.\n",
    "\n",
    "Mientras mas \"familiar\" sea el entorno, menos iteraciones por episodio y menos penalizaciones se observan (de hecho, no se registran), converge mas rapido a la nueva politica optima.\n",
    "\n",
    "De todas formas, el agente es capaz de aprender en entornos completamente desconocidos. Esto se ve reflejado en la disminución de la cantidad de iteraciones por episodio y la cantidad de penalizaciones a medida que el agente aprende. Hay unas iteraciones iniciales (alrededor de 600) donde el agente se comporta de manera errática, pero luego converge a la politica optima.\n",
    "\n",
    "Lo que si no parece ser una diferencia significativa es el hecho de conservar una de las dos paradas de la semilla original, se desempeñan de forma similar en ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4\n",
    "\n",
    "Realizar los cambios necesarios para que el agente sea capaz de tener un buen desempeño utilizando una semilla arbitraria, ejecutar iteraciones con semillas arbitrarias y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL2(AgenteRL):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__(entorno)\n",
    "        self.visits = {}\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        return super().elegir_accion(estado, max_accion)\n",
    "        \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        classic_update = recompensa + self.gamma * Q_max_estado_siguiente\n",
    "        tupla = (estado_anterior, accion)\n",
    "        \n",
    "        self.visits.setdefault(tupla, 0)\n",
    "        self.visits[tupla] += 1\n",
    "        self.k = self.visits[tupla]\n",
    "        \n",
    "        alpha_n =  1 / (1 + self.visits[tupla])\n",
    "        \n",
    "        self.Q[estado_anterior, accion] = (1 - alpha_n) * self.Q[estado_anterior, accion] + \\\n",
    "                                          (alpha_n) * classic_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_1 = AgenteRL(entorno)\n",
    "agente_2 = AgenteRL2(entorno)\n",
    "semilla = 50826476\n",
    "num_iteraciones_episodios = [[], []]\n",
    "num_penalizaciones_episodios = [[], []]\n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    j = i\n",
    "  else:\n",
    "    j = semilla + (i % 20)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente_2, j)\n",
    "  num_iteraciones_episodios[0] += [num_iteraciones]\n",
    "  num_penalizaciones_episodios[0] += [penalizaciones]\n",
    "  \n",
    "  num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente_1, j)\n",
    "  num_iteraciones_episodios[1] += [num_iteraciones]\n",
    "  num_penalizaciones_episodios[1] += [penalizaciones]\n",
    "\n",
    "metricas(num_iteraciones_episodios[0], num_penalizaciones_episodios[0])\n",
    "metricas(num_iteraciones_episodios[1], num_penalizaciones_episodios[1])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios[0]))], \n",
    "  num_iteraciones_episodios[0], \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios[0]))],\n",
    "  num_penalizaciones_episodios[0],\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios[1]))], \n",
    "  num_iteraciones_episodios[1], \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios[1]))],\n",
    "  num_penalizaciones_episodios[1],\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
