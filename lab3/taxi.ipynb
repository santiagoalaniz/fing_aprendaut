{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial del Framework Gymnasium y Herramientas de Trabajo proporcionadas por la cátedra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str.strip())\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacia arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "print()\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos, delay=0.01):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(delay)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        \n",
    "    return iteraciones, penalizaciones, marcos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "\n",
    "for i in range(10):\n",
    "    num_iteraciones, _, _ = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)} iteraciones, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "| Nombre           | C.I     | Email                        |\n",
    "|----------------|-----------|------------------------------|\n",
    "| Santiago Alaniz| 5082647-6 | santiago.alaniz@fing.edu.uy  |\n",
    "| Bruno De Simone| 4914555-0 | bruno.de.simone@fing.edu.uy  |\n",
    "| María Usuca    | 4891124-3 | maria.usuca@fing.edu.uy      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "\n",
    "El ambiente donde nuestro agente aprende es definido por el framework de RL `gymnasium`, concretamente el ambiente `taxi-v3`. \n",
    "\n",
    "Despues de leer la seccion anterior y la [documentación del framework](https://gymnasium.farama.org/environments/toy_text/taxi/), listamos las siguientes observaciones relevantes.\n",
    "\n",
    "### Codificación y representación de estados\n",
    "\n",
    "En el curso vimos que una forma de representar el ambiente es mediante cadenas de Markov, donde cada estado es un nodo y cada acción es una transición entre nodos (con una función de recompensa asociada a la arista que los comunica). En este caso, la cantidad de estados es 500, y la cantidad de acciones es 6.\n",
    "\n",
    "Los estados se indexan segun la siguiente fórmula:\n",
    "\n",
    "```\n",
    "((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "```\n",
    "\n",
    "Entonces, para cada estado, se codifica la información de las paradas origen/destino. Nuestro agente, que va a utilizar el algoritmo de Q learning, debe actualizar valores en una matriz Q de 500x6, donde cada fila representa un estado y cada columna una accion. \n",
    "\n",
    "### Representación conceptual del ambiente\n",
    "\n",
    "El ambiente es un conjunto de cadenas markovianas disconexas identificadas principalmente por el par de paradas (origen, destino), otros factores que aportan son:\n",
    "\n",
    "- la ubicación del pasajero en el mapa, que puede ser en el taxi o en una de las paradas\n",
    "- posición del taxi en la grilla.\n",
    "\n",
    "Por ejemplo, consideremos un estado `Ei` de alguna de las cadenas markovianas disjuntas. Este estado tiene codificado en su numero identificador:\n",
    "- Un par de paradas (origen, destino).\n",
    "- Una ubicación del pasajero, representado supongamos por un booleano, 0 si esta en el taxi, 1 si esta en alguna parada.\n",
    "- Una ubicación del taxi en la grilla.\n",
    "\n",
    "El estado `Ei` tiene 6 acciones posibles, podemos afirmar que para cada acción tomada el estado resultante `Ej` tiene codificado en su numero identificador:\n",
    "- El mismo par de paradas (origen, destino), si no es el estado final.\n",
    "- Una ubicación del pasajero, 0 si esta en el taxi, 1 si esta en la parada origen. Si la acción es recoger/dejar pasajero, este valor podría cambiar.\n",
    "- Una ubicación del taxi en la grilla que puede ser nueva si la acción es alguna de las 4 posibles de movimiento.\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-ubicacion-pasajero.png\"/>\n",
    "  </p>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-destinos.png\"/>\n",
    "  </p>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "Programar las funciones de la clase `AgenteRL`, manteniendo cualquier función adicional necesaria en la misma clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clase `AgenteRL`**\n",
    "\n",
    "Esta implementación está fuertemente inspirada en los conceptos presentados en el libro de Mitchell, *Machine Learning* (1997) y las clases impartidas por el equipo docente.\n",
    "\n",
    "El agente interactúa con un entorno provisto por el cuerpo docente, donde debemos mover un taxi en un mapa para recoger y dejar pasajeros. El objetivo es que el taxi aprenda la política óptima, es decir, recoger y dejar al pasajero con la menor cantidad de pasos y con la menor cantidad de penalizaciones.\n",
    "\n",
    "##### **Variables de instancia**:\n",
    "- `self.gamma`: Un factor de descuento, generalmente denotado como gamma (`γ`), que determina cuánto valor le da el agente a las recompensas futuras en comparación con las inmediatas. Toma como valor constante `0.9`. A diferencia de `self.k` y `self.delta_t`, no cambia con el tiempo, aunque somos conscientes de que agentes más vanguardistas aplican técnicas de perfeccionamiento de este valor a lo largo del tiempo.\n",
    "\n",
    "- `self.k`: Es el atributo pivotal que redefine la función de distribución `X_s_a` utilizada para elegir la \"mejor\" acción posible. `self.k` es la función creciente `f(x)` a lo largo de las iteraciones del episodio. Esto último es una decisión de diseño para beneficiar a la exploración en etapas iniciales. Queremos que el agente explore el entorno y no se quede estancado en un mínimo local, por lo que le damos más peso a la exploración en etapas iniciales, y a medida que el agente va aprendiendo, le damos más peso a la explotación.\n",
    "\n",
    "- `self.Q`: Una tabla Q inicializada como una matriz de ceros. Las dimensiones de la matriz dependen del número de estados (`entorno.observation_space.n`) y el número de acciones (`entorno.action_space.n`) en el entorno en el que se encuentra el agente, `(Q: 500x6)`.\n",
    "\n",
    "##### **Métodos**:\n",
    "\n",
    "- `__init__(self, entorno) -> None`: El constructor de la clase que inicializa las variables de instancia.\n",
    "\n",
    "- `elegir_accion(self, estado, max_accion) -> int`: Define la función de distribución `X_s_a` para el valor de `self.K` actual. Luego, utilizando el metodo `np.random.choice` elige una acción con distribucion `X_s_a`.\n",
    "\n",
    "- `aprender(self, estado_anterior, estado_siguiente, accion, recompensa)`: Este método actualiza la tabla Q utilizando la ecuación de actualización (Bellman), tambien incrementa el valor de `self.k` para que el agente empiece a preponderar la informacion recabada por sobre la exploracion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        self.gamma = 0.9\n",
    "        self.k = 1\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "        # Metricas\n",
    "        self.episodes_iter = []\n",
    "        self.episodes_pen = []\n",
    "        self.episodes_frames = []\n",
    "\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Aumento k (factor de \"confianza\")\n",
    "        self.k += 1\n",
    "        \n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max_estado_siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos auxiliares para la presentacion de resultados.\n",
    "\n",
    "- `metricas(iteraciones, penalizaciones)`: media y desviación estandar de las iteraciones y penalizaciones.\n",
    "- `ver_episodio(frames, tiempo_total= 5)`: muestra el episodio en el entorno, con un tiempo de espera entre frames de `tiempo_total` segundos.\n",
    "- `dibujar_subgrafico(**args)`: dibuja un subgráfico con los parámetros pasados por argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "\n",
    "def metricas(agente: AgenteRL):\n",
    "  print(f\"\\\n",
    "[{agente.__class__.__name__}]\\n\\\n",
    "  [ITERACIONES]       mean: {np.mean(agente.episodes_iter)}, std: {np.std(agente.episodes_iter)}\\n\\\n",
    "  [PENALIZACIONES]    mean: {np.mean(agente.episodes_pen)}, std: {np.std(agente.episodes_pen)}\\n\\\n",
    "  \")\n",
    "\n",
    "\n",
    "def ver_episodio(marcos, tiempo_total= 5): print_frames(marcos, delay= tiempo_total/len(marcos))\n",
    "  \n",
    "def dibujar_subgrafico(episodios, datos, subindice, etiqueta_y, titulo, color='blue', escala_log_x=False, escala_log_y=False):\n",
    "    plt.subplot(3, 1, subindice)\n",
    "    plt.plot(episodios, datos, color=color)\n",
    "    \n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel(etiqueta_y)\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    if escala_log_x: plt.xscale('log')\n",
    "    if escala_log_y: plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "def comparar_agentes(agente_a: AgenteRL, agente_b: AgenteRL):\n",
    "    iter_a = np.array(agente_a.episodes_iter)\n",
    "    iter_b = np.array(agente_b.episodes_iter)\n",
    "\n",
    "    comparative = np.where(iter_a < iter_b)\n",
    "    comparative_comp = np.where(iter_a >= iter_b)\n",
    "\n",
    "    metricas(agente_a)\n",
    "    metricas(agente_b)\n",
    "\n",
    "    print(f\"{agente_a.__class__.__name__} >> {agente_b.__class__.__name__} : {100*len(comparative[0]) / len(iter_a)} %\")\n",
    "    print(f\"{agente_a.__class__.__name__} << {agente_b.__class__.__name__} : {100*len(comparative_comp[0]) / len(iter_a)} %\")\n",
    "\n",
    "    _index = [i for i in range(0, len(agente_a.episodes_iter))]\n",
    "\n",
    "    plt.figure(figsize=(8, 12))\n",
    "\n",
    "    dibujar_subgrafico(\n",
    "    _index, \n",
    "    agente_a.episodes_iter, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    agente_a.__class__.__name__ + \" Iteraciones por episodio (escala logarítmica y)\",\n",
    "    escala_log_y=True\n",
    "    )\n",
    "\n",
    "    dibujar_subgrafico(\n",
    "    _index, \n",
    "    agente_b.episodes_iter, \n",
    "    2, \n",
    "    'Iteraciones', \n",
    "    agente_b.__class__.__name__ + \" Iteraciones por episodio (escala logarítmica y)\",\n",
    "    escala_log_y=True\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "Analizar los resultados de una ejecución de mil episodios con el agente programado. Agregar un nuevo bloque de de texto discutiendo los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_1 = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente_1, semilla)\n",
    "    agente_1.episodes_iter += [num_iteraciones]\n",
    "    agente_1.episodes_pen += [penalizaciones]\n",
    "    agente_1.episodes_frames += [marcos]\n",
    "\n",
    "metricas(agente_1)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "_index = [i for i in range(0, len(agente_1.episodes_iter))]\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index, \n",
    "  agente_1.episodes_iter, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "  escala_log_x=True, \n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index,\n",
    "  agente_1.episodes_pen,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio (escala logarítmica x)',\n",
    "  escala_log_x=True,\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados\n",
    "\n",
    "En nuestro algoritmo, hemos implementado una **estrategia de exploración que evoluciona a lo largo del tiempo**, lo que ha demostrado ser efectivo en el proceso de aprendizaje por refuerzo. Inicialmente, favorecemos la exploración, al ponderar la selección de acciones en función del conocimiento actual del agente y variando el parámetro `k` con el número de iteraciones. Esto permite al agente realizar exploración intensiva al principio y luego cambiar gradualmente hacia la explotación de conocimientos adquiridos. \n",
    "\n",
    "Los vestigios de exploración se pueden ver en los \"picos\" iniciales en la cantidad de iteraciones por episodio, esto da cuenta de la exploración intensiva que realiza el agente al principio. A medida que el agente acumula conocimiento sobre el entorno, la cantidad de iteraciones por episodio disminuye, llegado a un punto en el tiempo donde el agente se vuelve más eficiente en su toma de decisiones y la cantidad de iteraciones por episodio se estabiliza, que además coincide con la convergencia de la política óptima.\n",
    "\n",
    "En nuestros resultados se puede observar que la **cantidad de penalizaciones disminuye** a medida que aumenta el número de episodios. Este comportamiento es esperado, ya que el agente va acumulando conocimiento sobre el entorno y aprende a evitar las acciones que conllevan penalizaciones. Además, el agente se vuelve más eficiente en su toma de decisiones, lo que se refleja en una disminución en la cantidad de iteraciones necesarias para alcanzar su objetivo.\n",
    "\n",
    "En **comparación con el agente aleatorio**, el agente de aprendizaje por refuerzo logra reducir considerablemente la cantidad de iteraciones requeridas para alcanzar sus objetivos. Esto subraya la eficacia del aprendizaje por refuerzo en la mejora del desempeño del agente y su capacidad para tomar decisiones más informadas a lo largo del tiempo.\n",
    "\n",
    "Para poder apreciar mejor la evolución del agente, se puede ejecutar el siguiente código que muestra una serie de episodios de interés.\n",
    "\n",
    "*Nota*:\n",
    "\n",
    "A lo largo del laboratorio la definición de un `self.k` lo \"suficientemente bueno\" para que el agente aprenda fue un problema. Encontramos que un `self.k` que crece linealmente con el tiempo es una buena solución. Sin embargo,exploramos los límites de esta técnica.\n",
    "\n",
    "Por ejemplo, si definimos `self.k` como una función constante, el agente no logra converger a la política óptima. Si `self.k = 1` la distribución es exactamente equiprobable, es decir es el agente aleatorio. Pero el otro componente de la distribución es la tabla Q, que se actualiza con la ecuación de Bellman. Si `self.k = 1` entonces la tabla Q no se actualiza, y el agente no aprende. Si `self.k > 1` y `self.Q[accion]` es lo suficientemente grande, la distribución sera preponderante para esa acción.\n",
    "\n",
    "Entonces, queremos un `self.k` que crezca a medida que el agente \"aprende\", a nuestro juicio, es correcto afirmar que a medida que las iteraciones aumentan, el agente conoce más. Por lo tanto, `self.k` debe crecer en función de las iteraciones de cada episodio\n",
    "\n",
    "- `self.k` lineal: convergencia para todos los casos observados\n",
    "- `self.k` logarítmico: convergencia para algunos casos observados\n",
    "- `self.k` constante: convergencia para algunos casos observados\n",
    "\n",
    "La conclusión es que `self.k` debe crecer con el tiempo, y que ese orden debe ser lineal o mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los episodios\n",
    "ver_episodio(agente_1.episodes_frames[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3\n",
    "\n",
    "Ejecutar 1000 episodios con una semilla diferente y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos.\n",
    "\n",
    "### Metodología\n",
    "\n",
    "Tomando la siguiente proporción de ejecuciones, realizamos el siguiente experimento:\n",
    "\n",
    "- 60% con la misma semilla (SI). Para cada una de las variantes\n",
    "  - 40% con una semilla alternativa (SA) que difiera solo en la posición del taxi.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en la posición del taxi y en la posición del destino.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en todo.\n",
    "\n",
    "Es decir, tres subexperimentos, cada uno con 60% de ejecuciones con la misma semilla, y 40% de ejecuciones con una semilla alternativa, cada una genera un entorno más disrupitivo que la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metodologia = {\n",
    "  'taxi': (1, 9),\n",
    "  'taxi_destino': (1, 18),\n",
    "  'taxi_origen_destino': (1, 7),\n",
    "}\n",
    "\n",
    "for metodo in metodologia.items():\n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  for seed in metodo[1]:\n",
    "    entorno.reset(seed = seed)\n",
    "    print(f'SEED {seed}')\n",
    "    print_env(entorno)\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metodo in metodologia.items():\n",
    "  agente_1 = AgenteRL(entorno)\n",
    "  num_iteraciones_episodios = []\n",
    "  num_penalizaciones_episodios = []\n",
    "  \n",
    "  for i in range(600):\n",
    "    entorno.reset(seed = metodo[1][0])\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente_1, metodo[1][0])\n",
    "    agente_1.episodes_iter += [num_iteraciones]\n",
    "    agente_1.episodes_pen += [penalizaciones]\n",
    "  \n",
    "  for i in range(400):\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente_1, metodo[1][1])\n",
    "    agente_1.episodes_iter += [num_iteraciones]\n",
    "    agente_1.episodes_pen += [penalizaciones]\n",
    "  \n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  metricas(agente_1)\n",
    "\n",
    "  _index = [i for i in range(0, len(agente_1.episodes_iter))]\n",
    "  plt.figure(figsize=(15,10))\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    _index, \n",
    "    agente_1.episodes_iter, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica y)',\n",
    "    escala_log_y=True\n",
    "  )\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    _index,\n",
    "    agente_1.episodes_pen,\n",
    "    2,\n",
    "    'Penalizaciones',\n",
    "    'Penalizaciones por episodio',\n",
    "    color='red',\n",
    "  )\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados\n",
    "\n",
    "El agente sobreaprende un entorno, al cambiar la semilla, este cambia y el agente tiene que readaptarse. Esto se ve reflejado en la cantidad de iteraciones por episodio y la cantidad de penalizaciones.\n",
    "\n",
    "Además, como también intuíamos, el agente se comporta mejor mientras más parecido sea el entorno al que aprendió.\n",
    "\n",
    "En particular, cuando solo cambia la posición del taxi no sufre penalizaciones, aunque hay un aumento marginal en las iteraciones para luego converger rápidamente, dado que el binomio origen/destino es el mismo.\n",
    "\n",
    "Cuando ya cambia una de las posiciones de origen/destino, el agente sufre penalizaciones e iteraciones de forma similar al principio, tiene sentido ya que al cambiar una de las posiciones, se reconfigura totalmente el problema a resolver.\n",
    "\n",
    "No hay diferencias significativas entre cambiar una o ambas paradas de origen/destino respecto al ambiente original, se comportan de forma similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4\n",
    "\n",
    "Realizar los cambios necesarios para que el agente sea capaz de tener un buen desempeño utilizando una semilla arbitraria, ejecutar iteraciones con semillas arbitrarias y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos.\n",
    "\n",
    "### Metodología.\n",
    "\n",
    "En la parte anterior, vimos que el agente se comporta peor en entornos que difieren al entorno en el cual fue entrenado.\n",
    "\n",
    "Lo que hace este experimento es profundizar en este concepto, analizar como se comporta el agente cuando iteración a iteración se cambia la semilla, es decir, el entorno. Y analizar si al final de la iteración, el agente es capaz de adaptarse a cualquier semilla.\n",
    "\n",
    "Esquemáticamente, el experimento es el siguiente:\n",
    "\n",
    "- Se define una semilla de evaluación\n",
    "- Se define una semilla de entrenamiento \n",
    "- Se define un parámetro ciclos\n",
    "\n",
    "Para las primeras 600 iteraciones:\n",
    "\n",
    "- Se entrena el agente con una semilla x = semilla de entrenamiento + iteración\n",
    "\n",
    "Para las últimas 400 iteraciones\n",
    "\n",
    "- Se evalua el agente con una semilla x = semilla de evaluación + (iteracion % ciclos)\n",
    "\n",
    "Es decir, en primera istancia, el agente se entrena con 600 semillas potencialmente diferentes entre si, para luego evaluarlo con semillas que se repiten cada `ciclos` iteraciones. El objetivo es ver si el agente puede sobreponerse a la variabilidad del entorno para luego independientemente de la semilla, tener un buen desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempeño de nuestro agente `AgenteRL` en el experimento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_train = 50826476\n",
    "seed_test = 420\n",
    "cicles = 20\n",
    "\n",
    "agente_1 = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    x = seed_train + i\n",
    "  else:\n",
    "    x = seed_test + (i % cicles)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente_1, x)\n",
    "  agente_1.episodes_iter += [num_iteraciones]\n",
    "  agente_1.episodes_pen += [penalizaciones]\n",
    "  agente_1.episodes_frames += [marcos]\n",
    "\n",
    "metricas(agente_1)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "_index = [i for i in range(0, len(agente_1.episodes_iter))]\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index, \n",
    "  agente_1.episodes_iter, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index,\n",
    "  agente_1.episodes_pen,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestra implementación, el agente obviamente se comporta peor que en el experimento anterior, someterlo a un entorno diferente en cada iteración introduce ruido en el aprendizaje, haciendolo iterar más.\n",
    "\n",
    "Este ciclo de experimentación intensiva se puede apreciar en la gráfica, más precisamente entre los episodios 0-100.\n",
    "\n",
    "Pasado este ciclo, el agente logra identificar patrones en cada entorno y diferenciarlos de los demás, esto se ve en la reducción paulatina de iteraciones, las penalizaciones también son intensivas al principio, pero luego se anulan.\n",
    "\n",
    "Ya entre los episodios 400-600 podemos inferir debido al número de iteraciones y penalizaciones que el agente esta dando la política óptima para cada entorno.\n",
    "\n",
    "Lo que finalmente confirma nuestra sospecha es que a partir de la iteración 600, puede repetir el patrón definido por la `test_seed + (i % ciclos)`  para cada una de las 400 iteraciones restantes.\n",
    "\n",
    "Consideramos que el fenómeno que se observa puede ser explicado por la siguiente razonamiento:\n",
    "  - cada episodio nuevo, puede ser un entorno diferente al anterior, por lo tanto el agente tiene que aprender a adaptarse en ese entorno\n",
    "  - sin embargo, como vimos en la `parte 3)`, el agente se comporta mejor en entornos que se parecen.\n",
    "  - los entornos más dispares entre sí son los que tienen combinación de origen/destino diferentes `parte 3)`, por lo tanto el agente tiene que aprender a adaptarse a cada combinación de origen/destino\n",
    "  - los entornos donde solo cambia la posición del taxi, el agente simplemente tiene que aprender el camino más corto, no sufre penalizaciones, dado que el binomio origen/destino es el mismo, el aumento de iteraciones existe, pero marginal en comparación con los otros casos.\n",
    "\n",
    "Entonces, despreciando el escenario anterior, el problema se reduce a combinaciones origen/destino potencialmente diferentes en cada episodio. \n",
    "El número de pares únicos es `2 * C(4,2) = 12`, consideremos `P(x=par_i)`, la probabilidad de obtener el `par_i`. Asumiendo que gymnasium genera los pares de origen/destino de forma equiprobable, entonces P(x=par_i) = 1/12.\n",
    "\n",
    "`E(P(x=par_i)) = 1/12`, si repetimos el experimento **600 veces**, esperamos ver la misma combinación (origen, destino) **50 veces**, lo cual es suficiente para aprender, como vimos anteriormente `parte 1)`, con la misma semilla, el agente converge a la politca óptima en la 10ma iteración. \n",
    "\n",
    "En resumen, el agente es capaz de adaptarse a cualquier entorno, pero requiere de un periodo de aprendizaje intensivo, con la certeza de que en cada iteración, la esperanza matemática lo favorece, y pueda ver escenarios similares. Hay que destacar también el peso que tiene la `definición de estados explicada en la parte 1)` en el aprendizaje del agente, ya que el agente no tiene que aprender a adaptarse a cada entorno, sino a cada combinación de origen/destino, que sabemos que se modelan con cadenas markovianas disconexas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los episodios\n",
    "ver_episodio(agente_1.episodes_frames[600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potenciales Mejoras\n",
    "\n",
    "El agente `AgenteRL` es capaz de adaptarse a cualquier entorno, pero requiere de un periodo de aprendizaje intensivo de aproximadamente 100 episodios para luego poder identificar patrones en cada entorno y diferenciarlos de los demás. \n",
    "\n",
    "Este es el punto débil de nuestro agente, cualquier método alternativo que busque y logre reducir el periodo de aprendizaje intensivo, es una mejora potencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso de estudio: AgenteRL2 y su fracaso.\n",
    "\n",
    "Consideramos incluir en este obligatorio la implementacion de un agente que **nunca funcionó mejor** que el agente `AgenteRL`, pero que nos permitió explorar algunas ideas y familiarizarnos con conceptos que nos ayudaron a entender mejor el problema (fue clave para la redacción de la introducción y todas las partes en general).\n",
    "\n",
    "Aunque no pudimos rescatar ninguna heurística para mejorar al `AgenteRL` en términos de rendimiento. Consideramos que es importante incluirlo en este informe.\n",
    "\n",
    "En primera instancia habiamos entendido que, el hecho de que el **experimento varia de semilla**, puede ser visto como un problema donde el agente vive en un entorno donde **la función de recompensa es probabilistica**, es decir, la tupla (estado, acción) tiene una recompensa modelada por una variable aleatoria.\n",
    "\n",
    "Convenientemente, aunque también sesgados por querer aplicar conceptos del Mitchell. Al final del capítulo de `Reinforcement Learning`, se presenta un apartado denominado `Nondeterministic rewards and actions`, donde, intuíamos, habían conceptos que podían ser aplicados a este problema, obviamente, no fue así.\n",
    "\n",
    "La sección presenta un algoritmo que permite al agente adaptarse a un entorno donde la función de recompensa es una variable aleatoria. \n",
    "\n",
    "Modifica la ecuación de Bellman para que el agente pueda ponderar (con un termino denominado `alpha`) la recompensa recibida, en base a la cantidad de veces que se ha visto dicha recompensa, es decir la tríada (estado, acción, recompensa).\n",
    "\n",
    "El problema es que si bien el episodio cambia de semilla, y por lo tanto el ambiente, la naturaleza del mismo, generado por gymnassium, es, bajo ningún concepto, estocástico, la tupla (estado, acción) tiene siempre la misma recompensa **en todos los ambientes**, porque cada estado posible está debidamente identificado, así como cada acción posible (ver `parte 1)`).\n",
    "\n",
    "Entonces, aplicar el algoritmo de la sección `Nondeterministic rewards and actions` es un error conceptual, porque el agente no tiene que aprender a adaptarse a una recommpensa estocástica, sino a un entorno que cambia/mantiene el par (destino, origen) en cada iteración del experimento, pero que es determinístico.\n",
    "\n",
    "Fue un error de conceptos grave, pero nos permitió entender porque el agente `AgenteRL` funciona bien y abordar el problema desde una perspectiva más conceptual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL2(AgenteRL):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__(entorno)\n",
    "        self.visits = {}\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        return super().elegir_accion(estado, max_accion)\n",
    "        \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        classic_update = recompensa + self.gamma * Q_max_estado_siguiente\n",
    "        tupla = (estado_anterior, accion, recompensa)\n",
    "        \n",
    "        self.visits.setdefault(tupla, 0)\n",
    "        self.visits[tupla] += 1\n",
    "        \n",
    "        # Facr de confianza, definido en funcion de las visitas a la tupla\n",
    "        self.k += 1\n",
    "        \n",
    "        alpha_n =  1 / (1 + self.visits[tupla])\n",
    "        \n",
    "        self.Q[estado_anterior, accion] = (1 - alpha_n) * self.Q[estado_anterior, accion] + \\\n",
    "                                          (alpha_n) * classic_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_train = 50826476\n",
    "seed_test = 420\n",
    "cicles = 20\n",
    "\n",
    "agente_2 = AgenteRL2(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    x = seed_train + i\n",
    "  else:\n",
    "    x = seed_test + (i % cicles)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente_2, x)\n",
    "  agente_2.episodes_iter += [num_iteraciones]\n",
    "  agente_2.episodes_pen += [penalizaciones]\n",
    "  agente_2.episodes_frames += [marcos]\n",
    "\n",
    "metricas(agente_2)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "_index = [i for i in range(0, len(agente_2.episodes_iter))]\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index, \n",
    "  agente_2.episodes_iter, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  _index,\n",
    "  agente_2.episodes_pen,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los episodios\n",
    "ver_episodio(agente_2.episodes_frames[600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso de estudio: AgenteRL3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL3(AgenteRL):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__(entorno)\n",
    "        self.visits = {}\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        return super().elegir_accion(estado, max_accion)\n",
    "        \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        tupla = (estado_anterior, accion, recompensa)\n",
    "        \n",
    "        self.visits.setdefault(tupla, 0)\n",
    "        self.visits[tupla] += 1\n",
    "        self.k = self.visits[tupla] \n",
    "        \n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max_estado_siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_train = 50826476\n",
    "seed_test = 420\n",
    "cicles = 20\n",
    "\n",
    "agente_3 = AgenteRL3(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    x = seed_train + i\n",
    "  else:\n",
    "    x = seed_test + (i % cicles)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente_3, x)\n",
    "  agente_3.episodes_iter += [num_iteraciones]\n",
    "  agente_3.episodes_pen += [penalizaciones]\n",
    "  agente_3.episodes_frames += [marcos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los episodios\n",
    "ver_episodio(agente_3.episodes_frames[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparar_agentes(agente_3, agente_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
