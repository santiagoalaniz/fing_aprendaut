{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial del Framework Gymnasium y Herramientas de Trabajo proporcionadas por la cátedra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la biblioteca Gymnassium, que vamos a usar como framework de RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake gymnasium scipy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un ambiente y lo mostramos en pantalla. Para esto definimos una función para imprimir nuestro ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La semilla usada para crear el ambiente\n",
    "semilla = 1\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "# Una funcion de ayuda para imprimir el estado de nuestro mundo\n",
    "def print_env(estado):\n",
    "  env_str = estado.render()\n",
    "  print(env_str.strip())\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El rectángulo de color representa el taxi, amarillo cuando va sin pasajero y verde con pasajero.\n",
    "'|' representa una pared que el taxi no puede cruzar, es decir.\n",
    "R, G, Y, B son los puntos de interés, es decir, las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros, y la letra púrpura es el destino actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos la semilla, cambia el estado del ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una semilla diferente\n",
    "semilla = 2\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos el espacio de estados y de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Espacio de Acciones {entorno.action_space}\")\n",
    "print(f\"Espacio de Estados {entorno.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 6 acciones, que corresponden a:\n",
    " * 0 = ir al Sur\n",
    " * 1 = ir al Norte\n",
    " * 2 = ir al Este\n",
    " * 3 = ir al Oeste\n",
    " * 4 = recoger pasajero\n",
    " * 5 = dejar pasajero\n",
    "\n",
    "Los puntos cardinales siguen la convención Norte hacia arriba. Recoger/dejar al pasajero solo tienen efecto si el taxi está en la misma casilla que el pasajero, y en uno de los puntos de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente deberá elegir la acción a tomar en cada paso. Gymnassium nos expone funciones para esto. Si queremos movernos al sur, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "entorno.reset(seed = semilla)\n",
    "print_env(entorno)\n",
    "print()\n",
    "\n",
    "accion = 0 # Sur\n",
    "entorno.step(accion)\n",
    "\n",
    "print_env(entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para programar un agente. Empezando por uno random. Se puede ejecutar el codigo abajo varias veces para ver como cambia en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def episodio_random(semilla_ambiente = 1):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "    entorno.reset(seed = semilla_ambiente)\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # para la animación\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "\n",
    "    while not termino and not truncado:\n",
    "        #  selecciona una acción aleatoria del conjunto de todas las posibles acciones\n",
    "        accion = entorno.action_space.sample() \n",
    "        estado, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        iteraciones += 1\n",
    "\n",
    "\n",
    "    print(f\"Iteraciones: {iteraciones}\")\n",
    "    print(f\"Penalizaciones: {penalizaciones}\")\n",
    "\n",
    "    return marcos\n",
    "\n",
    "marcos = episodio_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el episodio completo abajo. Notar que seleccionamos la semillia de selector de acciones para que la corrida sea 'buena'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def print_frames(marcos, delay=0.01):\n",
    "    for i, marco in enumerate(marcos):\n",
    "        clear_output()\n",
    "        print(marco['marco'])\n",
    "        print(f\"Iteracion: {i + 1}\")\n",
    "        print(f\"Estado: {marco['estado']}\")\n",
    "        print(f\"Accion: {marco['accion']}\")\n",
    "        print(f\"Recompensa: {marco['recompensa']}\")\n",
    "        sys.stdout.flush()\n",
    "        # Aumentar este tiempo para ver mejor la animación\n",
    "        sleep(delay)\n",
    "\n",
    "print_frames(marcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora queremos programar un agente inteligente, para eso nos vamos a atener a la siguiente interfaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        \"\"\"Elegir la accion a tomar en el estado actual y el espacio de acciones\"\"\"\n",
    "        pass\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        \"\"\"Aprender a partir de la tupla \n",
    "            - estado_anterior: el estado desde que se empezó\n",
    "            - estado_siguiente: el estado al que se llegó\n",
    "            - accion: la acción que llevo al agente desde estado_anterior a estado_siguiente\n",
    "            - recompensa: la recompensa recibida en la transicion\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestro agente aleatorio, esto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AgenteAleatorio(Agente):\n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Elige una acción al azar\n",
    "        return random.randrange(max_accion)\n",
    "\n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # No aprende\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniendolo a jugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "semilla = 1\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "agente = AgenteAleatorio()\n",
    "\n",
    "iteraciones = 0\n",
    "penalizaciones, recompensa = 0, 0\n",
    "\n",
    "marcos = [] # for animation\n",
    "\n",
    "termino = False\n",
    "truncado = False\n",
    "estado_anterior, info = entorno.reset(seed = semilla)\n",
    "while not termino and not truncado:\n",
    "    # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "    accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "\n",
    "    # Realizamos la accion\n",
    "    estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "\n",
    "    # Le informamos al agente para que aprenda\n",
    "    agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "    # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "    if recompensa == -10:\n",
    "        penalizaciones += 1\n",
    "\n",
    "    # Put each rendered frame into dict for animation\n",
    "    marcos.append({\n",
    "        'marco': entorno.render(),\n",
    "        'estado': estado_siguiente,\n",
    "        'accion': accion,\n",
    "        'recompensa': recompensa\n",
    "        }\n",
    "    )\n",
    "\n",
    "    estado_anterior = estado_siguiente\n",
    "    iteraciones += 1\n",
    "\n",
    "print(f\"Iteraciones: {iteraciones}\")\n",
    "print(f\"Penalizaciones: {penalizaciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos encapsular lo anterior en una función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_episodio(agente, semilla):\n",
    "    entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "    iteraciones = 0\n",
    "    penalizaciones, recompensa = 0, 0\n",
    "\n",
    "    marcos = [] # for animation\n",
    "\n",
    "    termino = False\n",
    "    truncado = False\n",
    "    estado_anterior, info = entorno.reset(seed = semilla)\n",
    "    while not termino and not truncado:\n",
    "        # Le pedimos al agente que elija entre las posibles acciones (0..entorno.action_space.n)\n",
    "        accion = agente.elegir_accion(estado_anterior, entorno.action_space.n)\n",
    "        # Realizamos la accion\n",
    "        estado_siguiente, recompensa, termino, truncado, info = entorno.step(accion)\n",
    "        # Le informamos al agente para que aprenda\n",
    "        agente.aprender(estado_anterior, estado_siguiente, accion, recompensa)\n",
    "\n",
    "        # El agente trato de dejar/recoger al pasajero incorrectamente\n",
    "        if recompensa == -10:\n",
    "            penalizaciones += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        marcos.append({\n",
    "            'marco': entorno.render(),\n",
    "            'estado': estado_siguiente,\n",
    "            'accion': accion,\n",
    "            'recompensa': recompensa\n",
    "            }\n",
    "        )\n",
    "\n",
    "        estado_anterior = estado_siguiente\n",
    "        iteraciones += 1\n",
    "        \n",
    "    return iteraciones, penalizaciones, marcos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y correrlo varias veces para ver el rendimiento promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteAleatorio()\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "\n",
    "for i in range(10):\n",
    "    num_iteraciones, _, _ = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener métricas al respecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "print(f\"Se realizaron {numpy.mean(num_iteraciones_episodios)} iteraciones, en promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 3, Grupo 02 - Aprendizaje por Refuerzos\n",
    "\n",
    "| Nombre           | C.I     | Email                        |\n",
    "|----------------|-----------|------------------------------|\n",
    "| Santiago Alaniz| 5082647-6 | santiago.alaniz@fing.edu.uy  |\n",
    "| Bruno De Simone| 4914555-0 | bruno.de.simone@fing.edu.uy  |\n",
    "| María Usuca    | 4891124-3 | maria.usuca@fing.edu.uy      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduccion\n",
    "\n",
    "\n",
    "El modelado de la realidad en la cual se entrena al agente es definido por el framework `gymnasium`, escenario `taxi`. Obtenido de la [documentación](https://gymnasium.farama.org/environments/toy_text/taxi/) además de las posibles acciones se tiene que la ubicación del pasajero y los posibles destinos se representan:\n",
    "\n",
    "**Ubicaciones del pasajero:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "* 4: En taxi\n",
    "\n",
    "**Destinos:**\n",
    "* 0: Rojo\n",
    "* 1: Verde\n",
    "* 2: Amarillo\n",
    "* 3: Azul\n",
    "\n",
    "También de la documentación del framework vemos que define los estados con la siguiente ecuación:\n",
    "\n",
    "```\n",
    "((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination\n",
    "```\n",
    "\n",
    "Se observa que parte de lo que define un estado, y por lo cual se podría considerar que el taxi tiene visión global del escenario en el que se encuentra, es la ubicación del pasajero y su destino. Dado que se conoce el destino la cadena de Markov de la cual se quiere conocer la política óptima, sabemos que la misma es no conexa ya que dado cualquier estado para el cual el destino sea `Rojo` nunca se va a poder pasar a un estado para el cual el destino sea `Verde`.\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-destinos.png\"/>\n",
    "  </p>\n",
    "<br>\n",
    "\n",
    "Además, al saber la ubicación del pasajero se observa que dado un destino `X` y una ubicación `Y` distinta de 4, que significa que el pasajero está en el taxi, solo se puede pasar a un estado que tenga el mismo destino y ubicación O a uno que tenga el mismo destino y ubicación 4. Además, si se tiene ubicación de pasajero 4 nunca va a poder cambiar la ubicación del pasajero.\n",
    "\n",
    "<br>\n",
    "  <p align=\"center\">\n",
    "    <img width=\"400\" src=\"img/diagrama-ubicacion-pasajero.png\"/>\n",
    "  </p>\n",
    "<br>\n",
    "\n",
    "Se ve entonces que si entrenamos en un escenario cuyo destino es `X` luego para cualquier escenario en el cual el destino sea distinto de `X` el taxista no va a saber qué hacer sin entrenamiento ya que nunca pasó por dicho estado ni va a pasar por ningún estado en el cual el destino sea `X`. Lo mismo ocurre al entrenar para un escenario con ubicación inicial del pasajero `Y`, luego al estar en un escenario con ubicación inicial distinta hasta que no recoja al pasajero siempre va a pasar por estados desconocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "Programar las funciones de la clase `AgenteRL`, manteniendo cualquier función adicional necesaria en la misma clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clase `AgenteRL`**\n",
    "\n",
    "Esta implementación está fuertemente inspirada en los conceptos presentados en el libro de Mitchell, *Machine Learning* (1997) y las clases impartidas por el equipo docente.\n",
    "\n",
    "El agente interactúa con un entorno provisto por el cuerpo docente, donde debemos mover un taxi en un mapa para recoger y dejar pasajeros. El objetivo es que el taxi aprenda la política óptima, es decir, recoger y dejar al pasajero con la menor cantidad de pasos y con la menor cantidad de penalizaciones.\n",
    "\n",
    "##### **Variables de instancia**:\n",
    "- `self.gamma`: Un factor de descuento, generalmente denotado como gamma (`γ`), que determina cuánto valor le da el agente a las recompensas futuras en comparación con las inmediatas. Toma como valor constante `0.9`. A diferencia de `self.k` y `self.delta_t`, no cambia con el tiempo, aunque somos conscientes de que agentes más vanguardistas aplican técnicas de perfeccionamiento de este valor a lo largo del tiempo.\n",
    "\n",
    "- `self.delta_t`: Es una discretización de las acciones tomadas en un episodio. Se inicializa en 0 y se incrementa en 1 cada vez que el agente toma una acción.\n",
    "\n",
    "- `self.k`: Es el atributo pivotal que re-define la función de distribución `X_s_a` que se utiliza para elegir una acción. `self.k` es la función creciente `f(x) = 1 + log10(delta_t)`, con `delta_t = 0` vale `1` y aumenta en orden logarítmico. Esto último es una decisión de diseño para beneficiar a la exploración en etapas iniciales. Queremos que el agente explore el entorno y no se quede estancado en un mínimo local, por lo que le damos más peso a la exploración en etapas iniciales, y a medida que el agente va aprendiendo, le damos más peso a la explotación.\n",
    "\n",
    "- `self.Q`: Una tabla Q inicializada como una matriz de ceros. Las dimensiones de la matriz dependen del número de estados (`entorno.observation_space.n`) y el número de acciones (`entorno.action_space.n`) en el entorno en el que se encuentra el agente.\n",
    "\n",
    "##### **Métodos**:\n",
    "\n",
    "- `__init__(self, entorno) -> None`: El constructor de la clase que inicializa las variables de instancia.\n",
    "\n",
    "- `elegir_accion(self, estado, max_accion) -> int`: Define la función de distribución `X_s_a` para el valor de `self.K` actual, tal cual presentada en el libro del curso. Luego, utilizando el metodo `np.random.choice` elige una acción de acuerdo a la función de distribución `X_s_a` y la devuelve.\n",
    "\n",
    "- `aprender(self, estado_anterior, estado_siguiente, accion, recompensa)`: Este método actualiza la tabla Q utilizando la ecuación de actualización (Bellman). Aquí `Q_max` es el valor máximo de Q para el `estado_siguiente`. La tabla Q para el `estado_anterior` y la `accion` tomada se actualiza considerando la `recompensa` recibida y el valor descontado de `Q_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL(Agente):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = 0.9\n",
    "        self.k = 1\n",
    "        self.Q = np.zeros((entorno.observation_space.n, entorno.action_space.n))\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        # Exploracion vs Explotacion, Mitchell 97. p.379\n",
    "        aux = np.power(np.ones(max_accion) * self.k , self.Q[estado])\n",
    "        X_s_a = aux / np.sum(aux)\n",
    "        \n",
    "        # Elegimos una accion con distribucion X_s_a\n",
    "        return np.random.choice(max_accion, 1, p= X_s_a)[0]\n",
    "    \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Aumento k (factor de \"confianza\")\n",
    "        self.k += 1\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        self.Q[estado_anterior, accion] = recompensa + self.gamma * Q_max_estado_siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "\n",
    "def metricas(iteraciones, penalizaciones):\n",
    "  print(f\"[ITERACIONES]       mean: {np.mean(iteraciones)}, std: {np.std(iteraciones)}\")\n",
    "  print(f\"[PENALIZACIONES]    mean: {np.mean(penalizaciones)}, std: {np.std(penalizaciones)}\")\n",
    "  \n",
    "def dibujar_subgrafico(episodios, datos, subindice, etiqueta_y, titulo, color='blue', escala_log_x=False, escala_log_y=False):\n",
    "    plt.subplot(3, 1, subindice)\n",
    "    plt.plot(episodios, datos, color=color)\n",
    "    \n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel(etiqueta_y)\n",
    "    plt.title(titulo)\n",
    "    \n",
    "    if escala_log_x: plt.xscale('log')\n",
    "    if escala_log_y: plt.yscale('log')\n",
    "\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "Analizar los resultados de una ejecución de mil episodios con el agente programado. Agregar un nuevo bloque de de texto discutiendo los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente = AgenteRL(entorno)\n",
    "semilla = 1\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "for i in range(1000):\n",
    "    num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, semilla)\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "    marcos_episodios += [marcos]\n",
    "\n",
    "metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "  num_iteraciones_episodios, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica x,y)', \n",
    "  escala_log_x=True, \n",
    "  escala_log_y=True\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "  num_penalizaciones_episodios,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio (escala logarítmica x)',\n",
    "  escala_log_x=True,\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de resultados\n",
    "\n",
    "En nuestro algoritmo, hemos implementado una **estrategia de exploración que evoluciona a lo largo del tiempo**, lo que ha demostrado ser efectivo en el proceso de aprendizaje por refuerzo. Inicialmente, favorecemos la exploración, al ponderar la selección de acciones en función del conocimiento actual del agente y variando el parámetro `k` con el número de iteraciones. Esto permite al agente realizar exploración intensiva al principio y luego cambiar gradualmente hacia la explotación de conocimientos adquiridos. \n",
    "\n",
    "Los vestigios de exploración se pueden ver en los \"picos\" iniciales en la cantidad de iteraciones por episodio, esto da cuenta de la exploración intensiva que realiza el agente al principio. A medida que el agente acumula conocimiento sobre el entorno, la cantidad de iteraciones por episodio disminuye, llegado a un punto en el tiempo donde el agente se vuelve más eficiente en su toma de decisiones y la cantidad de iteraciones por episodio se estabiliza, que además coincide con la convergencia de la política óptima.\n",
    "\n",
    "En nuestros resultados se puede observar que la **cantidad de penalizaciones disminuye** a medida que aumenta el número de episodios. Este comportamiento es esperado, ya que el agente va acumulando conocimiento sobre el entorno y aprende a evitar las acciones que conllevan penalizaciones. Además, el agente se vuelve más eficiente en su toma de decisiones, lo que se refleja en una disminución en la cantidad de iteraciones necesarias para alcanzar su objetivo.\n",
    "\n",
    "En **comparación con el agente aleatorio**, el agente de aprendizaje por refuerzo logra reducir considerablemente la cantidad de iteraciones requeridas para alcanzar sus objetivos. Esto subraya la eficacia del aprendizaje por refuerzo en la mejora del desempeño del agente y su capacidad para tomar decisiones más informadas a lo largo del tiempo.\n",
    "\n",
    "Para poder apreciar mejor la evolución del agente, se puede ejecutar el siguiente código que muestra una serie de episodios de interés.\n",
    "\n",
    "*Nota*:\n",
    "\n",
    "A lo largo del laboratorio la definicion de un `self.k` lo \"suficientemente bueno\" para que el agente aprenda fue un problema. Encontramos que un `self.k` que crece linealmente con el tiempo es una buena solución. Sin embargo,exploramos los limites de esta tecnica.\n",
    "\n",
    "Por ejemplo, si definimos `self.k` como una función constante, el agente no logra converger a la política óptima. Si `self.k = 1` la distribucion es exactamente equiprobable, es decir es el agente aleatorio. Pero el otro componente de la distribución es la tabla Q, que se actualiza con la ecuación de Bellman. Si `self.k = 1` entonces la tabla Q no se actualiza, y el agente no aprende. Si `self.k > 1` y `self.Q[accion]` es lo suficientemente grande, la distribucion sera preponderante para esa accion.\n",
    "\n",
    "Entonces, queremos un `self.k` que crezca a medida que el agente \"aprende\", a nuestro juicio, es correcto afirmar que a medida que las iteraciones aumentan, el agente conoce mas. Por lo tanto, `self.k` debe crecer en funcion de las iteraciones de cada episodio\n",
    "\n",
    "- `self.k` lineal: convergencia para todos los casos observados\n",
    "- `self.k` logaritmico: convergencia para algunos casos observados\n",
    "- `self.k` constante: convergencia para algunos casos observados\n",
    "\n",
    "La conclusión es que `self.k` debe crecer con el tiempo, y que ese orden debe ser lineal o mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los marcos\n",
    "marco_interes = marcos_episodios[999]\n",
    "print_frames(marco_interes, delay=6/len(marco_interes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3\n",
    "\n",
    "Ejecutar 1000 episodios con una semilla diferente y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos.\n",
    "\n",
    "### Metodologia\n",
    "\n",
    "En el código anterior, el ambiente en el cual nuestro agente realiza sus tareas, formalmente, es un **DMDP, Deterministic Markov Decision Process**.\n",
    "\n",
    "Ahora, si cambiamos la semilla en medio del aprendizaje, el episodio tambien, dado que se genera un entorno diferente.\n",
    "\n",
    "En particular, **la posición del taxi y las paradas de origen/destino son aleatorias** (aunque no su posición en la grilla); todo lo demás se mantiene constante.\n",
    "\n",
    "Proponemos entonces, para analizar el comportamiento del agente variando la semilla, tomando una semilla inicial, y luego variando la semilla en cada episodio, de forma tal que el agente se enfrente a un entorno cada vez mas diferente.\n",
    "\n",
    "- (SI, SA) difieren solo en la posición del taxi.\n",
    "- (SI, SA) difieren en la posición del taxi y en la posición del origen.\n",
    "- (SI, SA) difieren en la posición del taxi y en la posición del destino.\n",
    "\n",
    "Tomando la siguiente proporción de ejecuciones, realizamos el siguiente experimento:\n",
    "\n",
    "- 60% con la misma semilla (SI). Para cada una de las variantes\n",
    "  - 40% con una semilla alternativa (SA) que difiera solo en la posición del taxi.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en la posición del taxi y en la posición del destino.\n",
    "  - 40% con una semilla alternativa (SA) que difiera en todo.\n",
    "\n",
    "Es decir, tres subexperimentos, cada uno con 60% de ejecuciones con la misma semilla, y 40% de ejecuciones con una semilla alternativa, cada una genera un entorno mas disrupitivo que la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metodologia = {\n",
    "  'taxi': (1, 9),\n",
    "  'taxi_destino': (1, 18),\n",
    "  'taxi_origen_destino': (1, 7),\n",
    "}\n",
    "\n",
    "entorno = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "\n",
    "for metodo in metodologia.items():\n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  for seed in metodo[1]:\n",
    "    entorno.reset(seed = seed)\n",
    "    print(f'SEED {seed}')\n",
    "    print_env(entorno)\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metodo in metodologia.items():\n",
    "  agente = AgenteRL(entorno)\n",
    "  num_iteraciones_episodios = []\n",
    "  num_penalizaciones_episodios = []\n",
    "  \n",
    "  for i in range(600):\n",
    "    entorno.reset(seed = metodo[1][0])\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente, metodo[1][0])\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "  \n",
    "  for i in range(400):\n",
    "    entorno.reset(seed = metodo[1][1])\n",
    "    num_iteraciones, penalizaciones, _ = ejecutar_episodio(agente, metodo[1][1])\n",
    "    num_iteraciones_episodios += [num_iteraciones]\n",
    "    num_penalizaciones_episodios += [penalizaciones]\n",
    "  \n",
    "  print(f\"Metodologia: {metodo[0]}\")\n",
    "  metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "  plt.figure(figsize=(15,10))\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "    num_iteraciones_episodios, \n",
    "    1, \n",
    "    'Iteraciones', \n",
    "    'Iteraciones por episodio (escala logarítmica y)',\n",
    "    escala_log_y=True\n",
    "  )\n",
    "\n",
    "  dibujar_subgrafico(\n",
    "    [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "    num_penalizaciones_episodios,\n",
    "    2,\n",
    "    'Penalizaciones',\n",
    "    'Penalizaciones por episodio',\n",
    "    color='red',\n",
    "  )\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de resultados\n",
    "\n",
    "El agente sobreaprende un entorno, al cambiar la semilla, el entorno cambia y el agente tiene que readaptarse. Esto se ve reflejado en la cantidad de iteraciones por episodio y la cantidad de penalizaciones.\n",
    "\n",
    "Ademas, como tambien intuiamos, el agente se comporta mejor en entornos que se parecen al entorno en el cual fue entrenado. \n",
    "\n",
    "En particular, cuando solo cambia la posición del taxi, el agente simplemente tiene que aprender el camino mas corto, no sufre penalizaciones, dado que el binomio origen/destino es el mismo, simplemente tiene que reaprender el camino mas corto en el nuevo entorno.\n",
    "\n",
    "Cuando ya cambia una de las posiciones de origen/destino, el agente sufre penalizaciones e iteraciones de forma similar al principio, tiene sentido ya que al cambiar una de las posiciones, se reconfigura la forma de obtener la recompensa maxima.\n",
    "\n",
    "En resumen, cuando solo cambia la posición del taxi, la forma de obtener la recompensa maxima, simplemente tiene que aprender el camino mas corto, al cambiar alguna de las paradas, es equivalente a volver a entrenar el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los marcos\n",
    "marco_interes = marcos_episodios[999]\n",
    "print_frames(marco_interes, delay=6/len(marco_interes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4\n",
    "\n",
    "Realizar los cambios necesarios para que el agente sea capaz de tener un buen desempeño utilizando una semilla arbitraria, ejecutar iteraciones con semillas arbitrarias y analizar los resultados. Agregar un nuevo bloque de texto discutiendo los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre el experimento a realizar\n",
    "\n",
    "En la parte anterior, vimos que el agente se comporta peor en entornos que difieren al entorno en el cual fue entrenado.\n",
    "\n",
    "Lo que hace este experimento es profundizar en este concepto, analizar como se comporta el agente cuando iteracion a iteracion se cambia la semilla, es decir, el entorno. Y analizar si al final de la iteracion, el agente es capaz de adaptarse a cualquier semilla.\n",
    "\n",
    "Esquematicamente, el experimento es el siguiente:\n",
    "\n",
    "```\n",
    "Se define una semilla de entrenamiento \n",
    "Se defina una semilla de evaluacion\n",
    "Se define un parametro ciclos\n",
    "\n",
    "Para las primeras 600 iteraciones\n",
    "Se entrena el agente con una semilla x = semilla de entrenamiento + iteracion\n",
    "Para las ultimas 400 iteraciones\n",
    "Se evalua el agente con una semilla x = semilla de evaluacion + (iteracion % ciclos)\n",
    "```\n",
    "\n",
    "Es decir, en primera istancia, el agente se entrena con 600 semillas potencialmente diferentes entre si, para luego evaluarlo con semillas que se repiten cada `ciclos` iteraciones.\n",
    "\n",
    "El objetivo es ver si el agente puede sobreponerse a la variabilidad del entorno para luego independientemente de la semilla, tener un buen desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempeño de nuestro agente `AgenteRL` en el experimento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_train = 50826476\n",
    "seed_test = 420\n",
    "cicles = 20\n",
    "\n",
    "agente = AgenteRL(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    x = seed_train + i\n",
    "  else:\n",
    "    x = seed_test + (i % cicles)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, x)\n",
    "  num_iteraciones_episodios += [num_iteraciones]\n",
    "  num_penalizaciones_episodios += [penalizaciones]\n",
    "  marcos_episodios += [marcos]\n",
    "\n",
    "metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "  num_iteraciones_episodios, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True,\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "  num_penalizaciones_episodios,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestra implementacion, el agente obviamente se comporta peor que en el experimento anterior, someterlo a un entorno diferente en cada iteracion introduce ruido en el aprendizaje, haciendolo iterar mas.\n",
    "\n",
    "Este ciclo de experimentacion intensiva se puede apreciar en la grafica, mas precisamente entre los episodios 0-100.\n",
    "\n",
    "Pasado este ciclo, el agente logra identificar patrones en cada entorno y diferenciarlos de los demas, esto se ve en la reduccion paulatina de iteraciones, las penalizaciones tambien son intensivas al principio, pero luego se anulan.\n",
    "\n",
    "Ya entre los episodios 400-600 podemos inferir debido al numero de iteraciones y penalizaciones que el agente esta dando la politica optima para cada entorno.\n",
    "\n",
    "Lo que finalmente confirma nuestra sospecha es que a partir de la iteracion 600, puede repetir el patron definido por la `test_seed + (i % ciclos)`  para cada una de las 400 iteraciones restantes.\n",
    "\n",
    "Consideramos que el fenomeno que se observa puede ser explicado por la siguiente razonamiento:\n",
    "  - cada episodio nuevo, puede ser un entorno diferente al anterior, por lo tanto el agente tiene que aprender a adaptarse en ese entorno\n",
    "  - sin embargo, como vimos en la parte 3, el agente se comporta mejor en entornos que se parecen.\n",
    "  - los entornos mas dispares entre si son los que tienen combinacion de origen/destino diferentes, por lo tanto el agente tiene que aprender a adaptarse a cada combinacion de origen/destino\n",
    "  - los entornos donde solo cambia la posicion del taxi, el agente simplemente tiene que aprender el camino mas corto, no sufre penalizaciones, dado que el binomio origen/destino es el mismo, el aumento de iteraciones existe, pero marginal en comparacion con los otros casos.\n",
    "\n",
    "Entonces, el ambiente tiene cuatro paradas de origen/destino, el numero de pares unicos es C(4,2) = 6, consideremos P(x=par), la probabilidad de obtener un par de origen/destino unico entre los 6 posibles.\n",
    "\n",
    "`E(P(x=par)) = 1/6`, si repetimos el experimento 600 veces, esperamos ver 100 veces cada par de origen/destino, esto asumiendo que gymnasium genera los pares de origen/destino de forma uniforme.\n",
    "\n",
    "\n",
    "En resumen, el agente es capaz de adaptarse a cualquier entorno, pero requiere de un periodo de aprendizaje intensivo para luego poder identificar patrones en cada entorno y diferenciarlos de los demas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver alguno de los marcos\n",
    "marco_interes = marcos_episodios[999]\n",
    "print_frames(marco_interes, delay=6/len(marco_interes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potenciales Mejoras\n",
    "\n",
    "El agente `AgenteRL` es capaz de adaptarse a cualquier entorno, pero requiere de un periodo de aprendizaje intensivo de aproximadamente 100 episodios para luego poder identificar patrones en cada entorno y diferenciarlos de los demas. \n",
    "\n",
    "Este es el punto debil de nuestro agente, cualquier metodo alternativo que busque y logre reducir el periodo de aprendizaje intensivo, es una mejora potencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso de estudio: AgenteRL2 y su fracaso.\n",
    "\n",
    "Consideramos incluir en este obligatorio la implementacion de un agente que nunca funciono mejor que el agente `AgenteRL`, pero que nos permitio explorar algunas ideas y familiarizarnos con conceptos que nos ayudaron a entender mejor el problema. Aunque no pudimos rescatar ninguna idea de este agente, consideramos que es importante incluirlo en este informe.\n",
    "\n",
    "En primera instancia entendimos que, el hecho de que el experimento varia de semilla puede ser visto como un problema donde el agente tiene que aprender a adaptarse en un entorno donde la funcion de recompensa es probabilistica, es decir, el agente tiene que aprender a adaptarse a un entorno donde la funcion de recompensa es una variable aleatoria.\n",
    "\n",
    "Convenientemente, aunque tambien sesgados por querer aplicar conceptos del Mitchell. El final del capitulo de `Reinforcement Learning` presenta un apartado denominado `Nondeterministic rewards and actions`, donde, intuiamos, se presentaba un problema similar al nuestro.\n",
    "\n",
    "La seccion presenta un algoritmo que permite al agente adaptarse a un entorno donde la funcion de recompensa es una variable aleatoria, modifica la ecuacion de Bellman para que el agente pueda ponderar la recompensa recibida en base a la cantidad de veces que se ha visto dicha recompensa, esto es la triada (estado, accion, recompensa).\n",
    "\n",
    "El problema es que si bien el episodio cambiaba de semilla, la naturaleza del entorno generado por gymnassium es, bajo ningun concepto, estocastico, el par (estado, accion) tiene siempre la misma recompensa, porque como vimos, en la parte 1, la matriz Q esta generada por todos los estados y acciones posibles, y aunque el agente no conozca el estado, este mismo se codifica/representa de forma tal que se puede obtener una vision global del entorno. (ver parte 1)\n",
    "\n",
    "Fue un error de conceptos grave, pero nos permitio entender porque el agente `AgenteRL` funciona bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Mitchell 97. Chapter 13\n",
    "class AgenteRL2(AgenteRL):\n",
    "    def __init__(self, entorno) -> None:\n",
    "        super().__init__(entorno)\n",
    "        self.visits = {}\n",
    "    \n",
    "    def elegir_accion(self, estado, max_accion) -> int:\n",
    "        return super().elegir_accion(estado, max_accion)\n",
    "        \n",
    "    def aprender(self, estado_anterior, estado_siguiente, accion, recompensa):\n",
    "        # Actualizamos la tabla Q con la ecuacion de Bellman \n",
    "        Q_max_estado_siguiente = np.max(self.Q[estado_siguiente])\n",
    "        classic_update = recompensa + self.gamma * Q_max_estado_siguiente\n",
    "        tupla = (estado_anterior, accion, recompensa)\n",
    "        \n",
    "        self.visits.setdefault(tupla, 0)\n",
    "        self.visits[tupla] += 1\n",
    "        # Facr de confianza, definido en funcion de las visitas a la tupla\n",
    "        self.k = self.visits[tupla]\n",
    "        \n",
    "        alpha_n =  1 / (1 + self.visits[tupla])\n",
    "        \n",
    "        self.Q[estado_anterior, accion] = (1 - alpha_n) * self.Q[estado_anterior, accion] + \\\n",
    "                                          (alpha_n) * classic_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_train = 50826476\n",
    "seed_test = 420\n",
    "cicles = 20\n",
    "\n",
    "agente = AgenteRL2(entorno)\n",
    "num_iteraciones_episodios = []\n",
    "num_penalizaciones_episodios = []\n",
    "marcos_episodios = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "for i in range(1000):\n",
    "  if i < 600:\n",
    "    x = seed_train + i\n",
    "  else:\n",
    "    x = seed_test + (i % cicles)\n",
    "    \n",
    "  num_iteraciones, penalizaciones, marcos = ejecutar_episodio(agente, x)\n",
    "  num_iteraciones_episodios += [num_iteraciones]\n",
    "  num_penalizaciones_episodios += [penalizaciones]\n",
    "  marcos_episodios += [marcos]\n",
    "\n",
    "metricas(num_iteraciones_episodios, num_penalizaciones_episodios)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_iteraciones_episodios))], \n",
    "  num_iteraciones_episodios, \n",
    "  1, \n",
    "  'Iteraciones', \n",
    "  'Iteraciones por episodio (escala logarítmica y)',\n",
    "  escala_log_y=True,\n",
    ")\n",
    "\n",
    "dibujar_subgrafico(\n",
    "  [i for i in range(0, len(num_penalizaciones_episodios))],\n",
    "  num_penalizaciones_episodios,\n",
    "  2,\n",
    "  'Penalizaciones',\n",
    "  'Penalizaciones por episodio',\n",
    "  color='red',\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
