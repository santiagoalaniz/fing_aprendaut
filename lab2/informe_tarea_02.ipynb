{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrega 2, Grupo 02 - Aprendizaje Bayesiano\n",
        "\n",
        "- Santiago Alaniz,  5082647-6, santiago.alaniz@fing.edu.uy\n",
        "- Bruno De Simone,  4914555-0, bruno.de.simone@fing.edu.uy\n",
        "- María Usuca,      4891124-3, maria.usuca@fing.edu.uy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivo\n",
        "\n",
        "Implementar un algoritmo de predicción de palabras utilizando Naive Bayes. El modelo se entrenará con datos de conversaciones de WhatsApp y se probará en un simulador de cliente. \n",
        "\n",
        "El algoritmo debe ser capaz de recomendar palabras basadas en las últimas N palabras ingresadas en una frase, donde N es un hiperparámetro que se variará para evaluar el desempeño del modelo. Además, el modelo se reentrenará al finalizar cada frase para adaptarse a nueva evidencia. \n",
        "\n",
        "La implementación debe ser eficiente en términos de uso de CPU, utilizando las estructuras de datos más adecuadas en Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diseño"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carga del dump de conversaciones en un grupo de WhatsApp\n",
        "\n",
        "En la letra del laboratorio se menciona que se puede acceder al dump en formato `.csv` pero esto es un error en la letra. *WhatsApp* exporta las conversaciones en formato `.txt`.\n",
        "\n",
        "Esto complejiza la carga de los datos ya que no podemos asumir que cada linea del archivo es una entrada del log.\n",
        "\n",
        "Detectamos el siguiente patron en el archivo de texto\n",
        "\n",
        "```\n",
        "  [dd/mm/aaaa hh:mm:ss] <nombre>: <mensaje>\n",
        "    ***\n",
        "  [dd/mm/aaaa hh:mm:ss] <nombre>: <mensaje>\n",
        "```\n",
        "\n",
        "Nos valemos del modulo `src.regex` para definir la expresion regular que detecta el patron y extrae los mensajes (`LOG_ENTRY_PATTERN`).\n",
        "\n",
        "`LOG_ENTRY_PATTERN = f'{metadata_pattern}{message_pattern}(?=\\n{metadata_pattern}|$)':`\n",
        "\n",
        "Esta expresion regular captura el contenido de cada entrada del log en dos grupos: `metadata` y `message`. \n",
        "\n",
        "- El grupo `metadata` captura la fecha y hora de la entrada, el nombre del usuario o el numero de telefono. \n",
        "- El grupo `message` captura el mensaje en si mismo y es el que nos interesa recuperar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from src.regex import LOG_ENTRY_PATTERN\n",
        "\n",
        "FILE_PATH = './assets/chat.txt'\n",
        "PATTERN = LOG_ENTRY_PATTERN\n",
        "\n",
        "with open(FILE_PATH, 'r', encoding='utf-8') as file:\n",
        "  data = file.read()\n",
        "\n",
        "matches = re.findall(PATTERN, data)\n",
        "data = [ match[1] for match in matches ]\n",
        "\n",
        "data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesamiento de datos\n",
        "\n",
        "Para un algoritmo de sugerencia de palabras basado en aprendizaje bayesiano, el preprocesamiento de los datos es crucial para obtener resultados significativos. En nuestro caso, se compone de los siguientes pasos:\n",
        "\n",
        "1. **Conversión a minúsculas**: Para que el algoritmo no trate las palabras como diferentes solo debido a las diferencias de mayúsculas y minúsculas.\n",
        "2. **Eliminación de caracteres especiales, numericos, emojis**: Estos componentes lexicos y semanticos no aportan información relevante para el modelo que busca sugerir palabras.\n",
        "3. **Eliminación de [stopwords](https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa)**: Estas palabras aparecen con mucha frecuencia en el lenguaje natural independientemente del contexto, por lo que no aportan información relevante para el modelo.\n",
        "4. **Tokenización ordenada**: Para que el algoritmo pueda identificar correctamente las palabras y su posición en la oración, es necesario dividir la oración en [tokens](https://es.wikipedia.org/wiki/Token).\n",
        "5. **Cruzamiento de palabras con un [corpus](http://universal.elra.info/product_info.php?cPath=42_43&products_id=1509)**: Para descartar palabras que no pertenecen al idioma español, chequeamos su existencia en un [corpus](https://es.wikipedia.org/wiki/Corpus_ling%C3%BC%C3%ADstico). Esto es importante para evitar que el modelo sugiera palabras en otros idiomas o palabras que no existen.\n",
        "6. **Relajar tildes**: Para que el modelo no distinga palabras con tildes de palabras sin tildes. Teniendo en cuenta que en los mensajes de WhatsApp es muy común que se omitan las tildes.\n",
        "\n",
        "Construimos la clase `src.G02Preprocessor` para encapsular el preprocesamiento de los datos y realizar los pasos mencionados anteriormente con mayor facilidad y control.\n",
        "\n",
        "*Nota:* \n",
        "\n",
        "La lista resultado del preprocesamiento tiene un tamaño menor a la lista original. Esto se debe a que el preprocesamiento descarta mensajes que no cumplen con los criterios de preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.preprocessing import G02Preprocessor\n",
        "\n",
        "preprocessor = G02Preprocessor()\n",
        "preprocessed_data = preprocessor.apply(data)\n",
        "\n",
        "preprocessed_data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algoritmo\n",
        "\n",
        "El algoritmo a implementar es un [clasificador bayesiano naive](https://es.wikipedia.org/wiki/Clasificador_bayesiano_ingenuo). Con la incorporación de un meta-pararametro `N`, que indica la cantidad de palabras anteriores a considerar para la predicción de la siguiente palabra. \n",
        "\n",
        "En la letra del laboratorio se menciona que este algoritmo es intensivo en CPU, es parte de la consigna del laboratorio implementar el algoritmo de la manera más eficiente posible.\n",
        "\n",
        "Detectamos las siguientes tareas para cumplir con la consigna:\n",
        "\n",
        "- **Definir las estructuras de datos**: Debemos diseñar las estructuras de datos que nos permitan almacenar la información necesaria para el funcionamiento del algoritmo. De forma tal que el acceso a la información y los calculos sean lo más eficientes posibles.\n",
        "- **Modulo auxiliar `naive_bayes_utils.py`**: Una vez diseñadas las estructuras de datos, debemos implementarlas, proponemos hacerlo siguiendo un enfoque modular, donde se plasmen las ideas discutidas en el punto anterior.\n",
        "- **Clase `G02NaiveBayesClassifier`**: Una vez implementadas las estructuras de datos, debemos implementar la clase `G02NaiveBayesClassifier` que encapsula el algoritmo de clasificación y mantiene todo el aparato auxiliar encapsulado bajo la misma clase. Esta clase será definida en el modulo `bayesian_learning.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Definir las estructuras de datos\n",
        "\n",
        "Nuestro modelo busca sugerir la palabra más probable dado un contexto de N palabras anteriores. Esto es coincidente con el objetivo mas general de los clasificadores bayesianos, que es encontrar la hipótesis más probable dado un conjunto de evidencias. (`h_map`)\n",
        "\n",
        "- `h_map` = argmax<sub>h</sub> P(h|D) = argmax<sub>h</sub> P(D|h) P(h) / P(D)\n",
        "\n",
        "Los clasificadores bayesianos naive asumen que las evidencias son independientes entre si, por lo que la probabilidad de la hipótesis MAP se puede expresar como el producto de las probabilidades de cada evidencia dada la hipótesis multiplicado por la probabilidad de la hipótesis.\n",
        "\n",
        "- `h_map` = argmax<sub>h</sub> P(D|h) P(h) / P(D) = argmax<sub>h</sub> P(D|h) P(h) = argmax<sub>h</sub> P(d<sub>1</sub>|h) ... P(d<sub>n</sub>|h) P(h)\n",
        "\n",
        "Nuestro meta-parámetro `N` indica la cantidad de palabras anteriores a considerar para la predicción de la siguiente palabra. Es decir, `N` es la cantidad de evidencias que consideramos para calcular la probabilidad de la hipótesis (la palabra a sugerir).\n",
        "\n",
        "Si bien un modelo bayesiano naive simplifica los calculos, diseñar estructuras de datos y algoritmos eficientes para calcular las probabilidades no es trivial. Proponemos las siguientes estructuras de datos para almacenar la información necesaria para el funcionamiento del algoritmo:\n",
        "\n",
        "- `V`: Vocabulario de palabras en el conjunto de entrenamiento.\n",
        "- `F_h`: Frecuencia de cada hipótesis (palabra) en el conjunto de entrenamiento.\n",
        "- `F_hD`: Frecuencia de cada evidencia (palabra anterior) dado una hipótesis (palabra) en el conjunto de entrenamiento. A diferencia de `F_h`, `F_hD` es un diccionario de diccionarios y es afectado por el meta-parámetro `N`, ya que la cantidad de evidencias a considerar depende de `N`.\n",
        "\n",
        "Para calcular las probabilidades usamos el m-estimador como estimador de máxima verosimilitud y suavizador de hipotesis que no aparecen en el conjunto de entrenamiento hasta el momento de la predicción.\n",
        "\n",
        "- `M`: Tomamos `M` como el numero de sobremuestras a incorporar en la estimacion de las probabilidades. Vamos a definir `M` como un hiperparámetro mas del modelo (con valor por defecto `M=1`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mover esto a un modulo en src\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def count_words(data, N=1):\n",
        "    F_h = Counter()\n",
        "    F_hD = defaultdict(Counter)\n",
        "\n",
        "    for sentence in data:\n",
        "        F_h.update(sentence)\n",
        "\n",
        "        for i in range(N, len(sentence)):\n",
        "            current_word = sentence[i]\n",
        "            previous_words = tuple(sentence[i-N:i])\n",
        "\n",
        "            for previous_word in previous_words:\n",
        "                F_hD[current_word].update([previous_word])\n",
        "\n",
        "    V = sum(F_h.values())\n",
        "\n",
        "    return V, F_h, F_hD\n",
        "\n",
        "V, F_h, F_hD = count_words(preprocessed_data)\n",
        "\n",
        "def p_hD(d, h, m=1):\n",
        "    p = 1 / V\n",
        "    F_hD_given_h = F_hD[h][d] if h in F_hD and d in F_hD[h] else 0\n",
        "    F_h_value = F_h[h] if h in F_h else 0\n",
        "\n",
        "    return (F_hD_given_h + m * p) / (F_h_value + m)\n",
        "\n",
        "def p_h(h, m=1):\n",
        "    p = 1 / V\n",
        "    F_h_value = F_h[h] if h in F_h else 0\n",
        "    \n",
        "    return (F_h_value + m * p) / (V + m)\n",
        "\n",
        "print(\"Número total de palabras:\", V)\n",
        "print(\"Frecuencias de palabras:\", F_h)\n",
        "print(\"Frecuencias de palabras dado el contexto:\", F_hD)\n",
        "print(\"Probabilidad de una palabra:\", p_h('hoy'))\n",
        "print(\"Probabilidad de una palabra dado el contexto:\", p_hD('disponibles', 'hoy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from collections import Counter\n",
        "\n",
        "TXT_PATH = './assets/chat.txt'\n",
        "\n",
        "nltk.download('words')\n",
        "spanish_words = set(words.words())\n",
        "\n",
        "with open(TXT_PATH, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "filtered_content = [line.split(':', 2)[-1].strip() for line in lines]\n",
        "all_words = ' '.join(filtered_content).split()\n",
        "all_words_lower = [word.lower() for word in all_words]\n",
        "\n",
        "\n",
        "filtered_words = [word for word in all_words_lower if word in spanish_words]\n",
        "\n",
        "word_count = Counter(filtered_words)\n",
        "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for word, frequency in sorted_words:\n",
        "    print(f\"{word}: {frequency}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluación\n",
        "- Qué conjunto de métricas se utilizan para la evaluación de la solución y su definición\n",
        "- Sobre qué conjunto(s) se realiza el entrenamiento, ajuste de la solución, evaluación, etc. Explicar cómo se construyen estos conjuntos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimentación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulador de cliente.\n",
        "\n",
        "El experimento consiste en evaluar los distintos hiperparametros del modelo. Alterando el tamaño de la ventana de palabras, y el vocabulario buscamos encontrar un equilibrio entre las siguientes propiedades deseables:\n",
        "\n",
        "- **Memoria**: El modelo debe ser capaz de sugerir palabras que tengan coherencia con el grupo de Whatsapp de donde se extrajeron los datos.\n",
        "\n",
        "- **Adaptacion**: El modelo debe ser capaz de adaptarse a la nueva evidencia, y mejorar su performance, sugiriendo palabras que tengan coherencia con la sesion de chat que se esta simulando.\n",
        "\n",
        "El siguiente script permite simular el comportamiento de un cliente que escribe frases, fue proporcionado por el cuerpo docente. Es de utilidad para evaluar el desempeño del modelo, ya que permite ingresar frases y ver las sugerencias que el modelo realiza.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrHEaPPcj30i",
        "outputId": "33cfb55d-d899-4720-c22b-71dfe9f51303"
      },
      "outputs": [],
      "source": [
        "def recomendacion_bayesiana(frase):\n",
        "  import random\n",
        "\n",
        "  dias = [\"Lunes\", \"Martes\", \"Miércoles\", \"Jueves\", \"Viernes\", \"Sábado\", \"Domingo\"]\n",
        "\n",
        "  # ## PSEUDOCODIGO - pag 55 - diapo##\n",
        "\n",
        "  # D = [\"mi\", \"hijo\", \"se\", \"olvidó\", \"de\", \"la\"]\n",
        "  # Horizonte = 4 #hiperparametro\n",
        "  # h_MAP = \"\"\n",
        "  # p_MAP = 0\n",
        "  # for h in P:\n",
        "    \n",
        "  #   prob = P[h]\n",
        "    \n",
        "  #   for d in D[-Horizonte:]:\n",
        "  #     prob = prob * PD[h].get(d, P_nada) \n",
        "  #     #P_nada es un valor que debemos definir para el caso cuando la palabra no se encuentre en el listado\n",
        "    \n",
        "  #   if prob > p_MAP:\n",
        "  #     h_MAP , p_MAP = h , prob\n",
        "  # print(h_MAP)\n",
        "\n",
        "  return(random.choice(dias))\n",
        "\n",
        "\n",
        "##### LOOP PRINCIPAL #####\n",
        "\n",
        "print(\"Ingrese la frase dando ENTER luego de \\x1b[3mcada palabra\\x1b[0m.\")\n",
        "print(\"Ingrese sólo ENTER para aceptar la recomendación sugerida\")\n",
        "print(\"Ingrese '.' para comenzar con una frase nueva.\")\n",
        "print(\"Ingrese '..' para terminar el proceso.\")\n",
        "\n",
        "frase = []\n",
        "palabra_sugerida = \"\"\n",
        "\n",
        "while 1:\n",
        "  palabra = input(\">> \")\n",
        "\n",
        "  if palabra == \"..\":\n",
        "    break\n",
        "\n",
        "  elif palabra == \".\":\n",
        "    print(\"----- Comenzando frase nueva -----\")\n",
        "    frase = []\n",
        "\n",
        "  elif palabra == \"\": # acepta última palabra sugerida\n",
        "    frase.append(palabra_sugerida)\n",
        "\n",
        "  else: # escribió una palabra\n",
        "    frase.append(palabra)\n",
        "\n",
        "  if frase:\n",
        "    palabra_sugerida = recomendacion_bayesiana(frase)\n",
        "\n",
        "    frase_propuesta = frase.copy()\n",
        "    frase_propuesta.append(\"\\x1b[3m\"+ palabra_sugerida +\"\\x1b[0m\")\n",
        "\n",
        "    print(\" \".join(frase_propuesta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una breve conclusión del trabajo realizado. Por ejemplo: \n",
        "- ¿cuándo se dieron los mejores resultados del jugador?\n",
        "- ¿encuentra alguna relación con los parámetros / oponentes/ atributos elegidos?\n",
        "- ¿cómo mejoraría los resultados?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
