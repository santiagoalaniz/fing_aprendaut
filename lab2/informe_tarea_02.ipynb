{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrega 2, Grupo 02 - Aprendizaje Bayesiano\n",
        "\n",
        "- Santiago Alaniz,  5082647-6, santiago.alaniz@fing.edu.uy\n",
        "- Bruno De Simone,  4914555-0, bruno.de.simone@fing.edu.uy\n",
        "- María Usuca,      4891124-3, maria.usuca@fing.edu.uy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivo\n",
        "\n",
        "Implementar un algoritmo de predicción de palabras utilizando Naive Bayes. El modelo se entrenará con datos de conversaciones de WhatsApp y se probará en un simulador de cliente. El algoritmo debe ser capaz de recomendar palabras basadas en las últimas `N` palabras ingresadas en una frase. `N` es un **hiperparámetro** a incorporar en el modelo. Además, el modelo se reentrenará al finalizar cada frase para adaptarse a nueva evidencia. \n",
        "<br>\n",
        "\n",
        "La implementación debe ser eficiente en términos de uso de CPU, utilizando las estructuras de datos más adecuadas en Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diseño"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carga del dump de conversaciones en un grupo de WhatsApp y Particionamiento\n",
        "\n",
        "WhatsApp permite exportar las conversaciones de un grupo en un archivo de texto plano. Cada entrada del dump tiene el siguiente formato:\n",
        "\n",
        "```\n",
        "  [dd/mm/aaaa hh:mm:ss] <nombre | telefono>: <mensaje>\n",
        "```\n",
        "\n",
        "Nos valemos del modulo `src.regex` para definir la [regex](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular) que detecta el patron. Esta expresión regular captura el contenido de cada entrada del dump en dos grupos:\n",
        "\n",
        "- `metadata`: metadata generada por el dump de WhatsApp. No nos interesa recuperarla.\n",
        "\n",
        "- `message`:  Mensajes de texto escritos por los usuarios del grupo. Se utiliza para entrenar el modelo.\n",
        "\n",
        "Particionamos los datos en tres conjuntos con `train_test_split` de `sklearn`:\n",
        "\n",
        "  - `train` para entrenar el modelo.\n",
        "\n",
        "  - `devel` para evaluar el modelo.\n",
        "\n",
        "  - `test` para evaluar el modelo final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from src.whatsapp_regex import LOG_ENTRY_PATTERN as PATTERN\n",
        "\n",
        "SEED = 42069\n",
        "random.seed(SEED)\n",
        "\n",
        "FILE_PATH = './assets/chat.txt'\n",
        "\n",
        "with open(FILE_PATH, 'r', encoding='utf-8') as f: data = f.read()\n",
        "matches = re.findall(PATTERN, data)\n",
        "data = [ match[1] for match in matches ]\n",
        "\n",
        "TEST_SIZE   = 0.5\n",
        "DEVEL_SIZE  = 0.5\n",
        "\n",
        "train, test = train_test_split(data, test_size= TEST_SIZE, random_state= SEED)\n",
        "test, devel = train_test_split(test, test_size= DEVEL_SIZE, random_state= SEED)\n",
        "\n",
        "print(f'\\\n",
        "Cantidad de mensajes: {len(data)}\\n\\\n",
        "Train, Devel, Test: ({len(train)}, {len(devel)}, {len(test)})\\n\\\n",
        "')\n",
        "\n",
        "train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesamiento de datos\n",
        "\n",
        "Para nuestro algoritmo de predicción de palabras optamos aplicar los siguientes pasos de preprocesamiento:\n",
        "\n",
        "- **Tokenización ordenada**: Para que el algoritmo pueda identificar correctamente las palabras y su posición en la oración, es necesario dividir la oración en [tokens](https://es.wikipedia.org/wiki/Token).\n",
        "\n",
        "- **Conversión a minúsculas**: Para que el algoritmo no trate las palabras como diferentes solo debido a las diferencias de mayúsculas y minúsculas.\n",
        "\n",
        "- **Eliminación de caracteres especiales, numéricos, emojis**: Estos componentes léxicos y semanticos no aportan información relevante para el modelo que busca sugerir palabras.\n",
        "\n",
        "- **Relajar tildes con [unidecode](https://pypi.org/project/Unidecode/)**: Para que el modelo no distinga palabras con tildes de palabras sin tildes. Teniendo en cuenta que en los mensajes de WhatsApp es muy común que se omitan las tildes.\n",
        "\n",
        "- **Validación de palabras en español**: Construimos un conjunto de palabras en español a partir de dos fuentes: `es_words.txt` y `nltk.corpus.stopwords('spanish')`. Esta colección se utiliza para filtrar palabras que no pertenecen al idioma.\n",
        "\n",
        "Construimos la clase `src.G02Preprocessor` para encapsular la lógica de preprocesamiento. Esta clase se encarga de aplicar los pasos de preprocesamiento mencionados anteriormente. Además, se encarga de almacenar las palabras en un diccionario para facilitar el acceso a las mismas.\n",
        "\n",
        "*Nota*:\n",
        "\n",
        "En el último punto asumimos que el conjunto `es_words.txt + nltk.corpus.stopwords('spanish')` es representativo del lenguaje. Teniendo en cuenta la extensión de este conjunto (`~600k palabras`), el hecho de que una palabra en español no pertenezca a este conjunto es un escenario marginal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.preprocessing import G02Preprocessor\n",
        "\n",
        "\n",
        "preprocessor = G02Preprocessor()\n",
        "ejemplo = random.choice(train)\n",
        "\n",
        "print(f'\\\n",
        "Prepocesador:\\n\\\n",
        "    Palabras del diccionario: {len(preprocessor.V_SPA)} \\n\\\n",
        "    Ejemplo:\\n      {ejemplo}\\n\\\n",
        "    Ejemplo preprocesad: {preprocessor.apply([ejemplo])}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algoritmo\n",
        "\n",
        "El algoritmo a implementar es un [clasificador bayesiano naive](https://es.wikipedia.org/wiki/Clasificador_bayesiano_ingenuo). Con la incorporación de un meta-pararametro `N`, que indica la cantidad de palabras anteriores a considerar para la predicción de la siguiente palabra. \n",
        "\n",
        "En la letra del laboratorio se menciona que este algoritmo es intensivo en CPU, es parte de la consigna del laboratorio implementar el algoritmo de la manera más eficiente posible.\n",
        "\n",
        "Objetivos a cumplir:\n",
        "\n",
        "- **Modulo auxiliar `naive_bayes_utils.py`**: Define las estructuras de datos auxiliares y operaciones matemáticas necesarias para el funcionamiento de un clasificador bayesiano, debemos definir funciones que las actualicen y consulten. Estas funciones se encuentran definidas en el modulo `naive_bayes_utils.py`\n",
        "\n",
        "- **Clase `G02NaiveBayesClassifier`**: Una vez implementadas las estructuras de datos, debemos implementar la clase `G02NaiveBayesClassifier` que encapsula el algoritmo de clasificación y mantiene todo el aparato auxiliar encapsulado bajo la misma clase. Esta clase esta definida en el modulo `bayesian_learning.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modulo auxiliar `naive_bayes_utils.py`\n",
        "\n",
        "El modulo `naive_bayes_utils.py` encapsula las estructuras de datos y los algoritmos auxiliares necesarios para el funcionamiento del clasificador bayesiano naive. \n",
        "\n",
        "Estas estructuras de datos fueron definidas en el punto anterior y fueron implementadas en el modulo `naive_bayes_utils.py` de la siguiente manera:\n",
        "\n",
        "- `build(data, N)`: Genera estructuras auxiliares para realizar el calculo de probabilidades. Recibe como parámetro el conjunto de entrenamiento y el meta-parámetro `N`. Devuelve las siguientes estructuras:\n",
        "\n",
        "    - `N_OF_WORDS`: Cantidad de palabras en el conjunto de entrenamiento.\n",
        "    \n",
        "    - `F_h`: Frecuencia de cada hipótesis (palabra) en el conjunto de entrenamiento.\n",
        "\n",
        "    - `F_hD`: Frecuencia de cada evidencia (palabra anterior) dado una hipótesis (palabra) en el conjunto de entrenamiento. `F_hD` es un diccionario de diccionarios y es afectado por el meta-parámetro `N`, ya que la cantidad de evidencias a considerar depende de `N`. Por ejemplo, si `N=4` y la hipótesis es `h`, `F_hD[h]` es un diccionario que contiene la frecuencia de cada combinación de 4 palabras anteriores a `h` en el conjunto de entrenamiento.\n",
        "\n",
        "- `p_h(h, V, F_h, data)`: Calcula una probabilidad ajustada para la ocurrencia de una palabra específica (`h`) en el conjunto de datos.\n",
        "- `p_hD(d, h, V_SPA, F_h, F_hD, m=1)`: Calcula la probabilidad condicional de que una palabra (`d`) preceda a (`h`).\n",
        "\n",
        "*Nota*:\n",
        "\n",
        "El enfoque de `build` es greedy. Si la frase no tiene el largo de palabras necesario toma hasta donde puede. Por ejemplo, si `N=4` y la frase es `['hola', 'como', 'estas']`, se toma `['hola', 'como']` como evidencia para `['estas']`, `['hola']` como evidencia para `['como']` y asi sucesivamente.\n",
        "\n",
        "En la firma de `p_h` abusamos de la notación, en realidad estamos calculando la métrica **tf-idf**. Consideramos que esta métrica es mucho mas significativa que simplemente la frecuencia de la hipótesis en el conjunto de entrenamiento. [TF-IDF](https://es.wikipedia.org/wiki/Tf-idf) es una métrica comúnmente usada en el procesamiento del lenguaje natural para valorar la importancia de una palabra en un corpus en relación con su frecuencia en documentos específicos. En este caso particular, si una palabra tiene una frecuencia alta y es muy común a lo largo de todo el corpus, es decir, aparece en muchas frases, entonces no es muy significativa para la predicción. Por el contrario, si una palabra tiene una frecuencia alta y aparece en pocas frases, entonces es muy significativa para la predicción. Es una forma elegante de reducir la inferencia de las [stopwords](https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa) en el modelo sin tener que filtrarlas.\n",
        "\n",
        "En la implementación de `p_hD` se utiliza un m-stimador para evitar que la probabilidad condicional sea cero. Asumimos equiprobabilidad a priori de la evidencia, siendo esta 1/len(V_SPA) donde V_SPA es el vocabulario de palabras en español que utilizamos en el preprocesamiento de los datos.\n",
        "\n",
        "En lo que uso de memoria refiere, consideramos más apropiado guardar las frecuencias de las hipótesis y las evidencias en un diccionario de diccionarios, en lugar de una matriz. Esto se debe a que la cantidad de hipótesis y evidencias es muy grande, y la cantidad de combinaciones de hipótesis y evidencias es mucho mayor. Por lo tanto, la matriz de frecuencias sería muy dispersa y ocuparía mucha memoria.\n",
        "\n",
        "En lo que uso de ciclos del cpu refiere, creemos que es más eficiente mantener las frecuencias y no las probabilidades, dado que a la hora de aprender una nueva frase, es menos costoso actualizar lo primero que lo segundo. La fórmula de `h_map` se hace 'on the fly' y no se guarda en memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.naive_bayes_utils import *\n",
        "\n",
        "_train = preprocessor.apply(train)\n",
        "\n",
        "N_OF_WORDS, F_h, F_hD = build(_train, N=1)\n",
        "\n",
        "print(f'\\\n",
        "Estructuras de datos\\n\\\n",
        "    Cantidad de palabras en train:    {N_OF_WORDS}\\n\\\n",
        "    Frecuencia de h:                  {(F_h)}\\n\\\n",
        "    Frecuencia condicional D dado h:  {(F_hD)}\\n')\n",
        "\n",
        "h_ex = random.choice(list(F_h.keys()))\n",
        "d_ex = random.choice(list(preprocessor.V_SPA))\n",
        "\n",
        "print(f'\\\n",
        "Probabilidades\\n\\\n",
        "    h_ex: {h_ex}\\n\\\n",
        "    d_ex: {d_ex}\\n\\\n",
        "    p_h(h_ex) (td-idf): {p_h(h_ex, N_OF_WORDS, F_h, _train)}\\n\\\n",
        "    p_hD(d_ex)(h_ex): {p_hD(d_ex, h_ex, preprocessor.V_SPA, F_h, F_hD)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Clase `G02NaiveBayesClassifier`\n",
        "\n",
        "La clase `G02NaiveBayesClassifier` es nuestra implementación del algoritmo de clasificación Naive Bayes con un enfoque en el procesamiento del lenguaje natural. Estos son los métodos principales de la clase:\n",
        "\n",
        "- **__init__(self, data, N=1, M=1)** Inicializa el clasificador:\n",
        "\n",
        "    - `data`: El conjunto de datos sobre el cual se construirá el modelo.\n",
        "    \n",
        "    - `N`: El tamaño del contexto para el modelo.\n",
        "    \n",
        "    - `M`: Un parámetro para suavizado.\n",
        "    \n",
        "    Se realiza la preprocesamiento de los datos y se construyen las estadísticas (frecuencias de términos y pares de términos) utilizando la función `build`.\n",
        "\n",
        "- **predict(self, sentence)** Realiza una predicción para una oración dada tomando en cuenta las N-últimas palabras:\n",
        "\n",
        "    - `sentence`: La oración para la cual se realizará la predicción.\n",
        "\n",
        "    Calcula la `h_map` para las N-últimas palabras de `sentence`. Si `h_map` es alguna de las N-últimas palabras de `sentence`, se itera hasta encontrar la palabra más probable que no sea alguna de las N-últimas palabras de `sentence`.\n",
        "\n",
        "- **update(self, new_sentence)** Actualiza el modelo con una nueva oración:\n",
        "\n",
        "    - `new_sentence`: La nueva oración para actualizar el modelo.\n",
        "\n",
        "    Aplica el mismo preprocesamiento de los datos iniciales a la `new_sentence` y actualiza las estadísticas del modelo (`F_h` y `F_hD`).\n",
        "\n",
        "- **self.preprocessor**: Instancia de la clase `G02Preprocessor` que encapsula la lógica de preprocesamiento.\n",
        "\n",
        "- **self.V_SPA**: Vocabulario de palabras en español que utilizamos en el preprocesamiento de los datos.\n",
        "\n",
        "- **self.N_OF_WORDS, self.F_h, self.F_hD**: Estructuras auxiliares para calcular las probabilidades de las hipótesis.\n",
        "\n",
        "*Nota*:\n",
        "\n",
        "La decisión de actualizar el Vocabulario en Español (`V_SPA`) con las palabras de la nueva oración es discutible. Por un lado, es posible que la nueva oración contenga palabras que no pertenecen al idioma español, como también que el usuario escriba palabras del español poco usuales y que estas no esten dentro del compendio del `cess_esp`. \n",
        "\n",
        "Por eso, asumimos que el usuario del cliente escribe en español (aunque se tome libertades a la hora de asignar tíldes)\n",
        "\n",
        "El siguiente código provee un benchmarking de la implementación del algoritmo de clasificación bayesiana naive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.bayesian_learning import G02NaiveBayesClassifier\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "clf = G02NaiveBayesClassifier(train, N=1)\n",
        "print(f'[INIT]  Tiempo de ejecución: {time.time() - start:.4f}s')\n",
        "test = [random.choice(list(preprocessor.V_SPA))]\n",
        "\n",
        "print(' '.join(test))\n",
        "\n",
        "start = time.time()\n",
        "for i in range(2): test.append(clf.predict(test))\n",
        "print(f'[PREDICT] Tiempo de ejecución: {time.time() - start:.4f}s')\n",
        "\n",
        "print(f'        V: {clf.N_OF_WORDS} \\\n",
        "    \\n          F_h[{test[1]}]: {clf.F_h[test[1]]} \\\n",
        "    \\n          F_hD[{test[1]}][{test[0]}]: {clf.F_hD[test[1]][test[0]]} \\\n",
        "    \\n          F_h[{test[2]}]: {clf.F_h[test[2]]} \\\n",
        "    \\n          F_hD[{test[2]}][{test[1]}]: {clf.F_hD[test[2]][test[1]]}')\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "print(' '.join(test))\n",
        "\n",
        "clf.update(' '.join(test))\n",
        "print(f'[UPDATE]  Tiempo de ejecución: {time.time() - start:.4f}s')\n",
        "\n",
        "print(f'        V: {clf.N_OF_WORDS} \\\n",
        "    \\n          F_h[{test[1]}]: {clf.F_h[test[1]]} \\\n",
        "    \\n          F_hD[{test[1]}][{test[0]}]: {clf.F_hD[test[1]][test[0]]} \\\n",
        "    \\n          F_h[{test[2]}]: {clf.F_h[test[2]]} \\\n",
        "    \\n          F_hD[{test[2]}][{test[1]}]: {clf.F_hD[test[2]][test[1]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluación\n",
        "\n",
        "En cualquier laboratorio o proyecto de investigación, la sección de evaluación es crucial para entender cómo se mide el rendimiento del modelo y qué métricas son relevantes para el problema en cuestión. Sin embargo, la naturaleza planteada en la letra del laboratorio no permite una evaluación cuantitativa del modelo. Por lo tanto, proponemos una evaluación cualitativa del modelo.\n",
        "\n",
        "Alterando el tamaño de la ventana de palabras `N`  buscamos encontrar un equilibrio entre las siguientes cualidades deseables:\n",
        "\n",
        "- **Memoria**: El modelo debe ser capaz de sugerir palabras que tengan coherencia con el grupo de Whatsapp de donde se extrajeron los datos.\n",
        "\n",
        "- **Adaptación**: El modelo debe ser capaz de adaptarse a la nueva evidencia, y mejorar su performance, sugiriendo palabras que tengan coherencia con la sesión de chat que se esta simulando.\n",
        "\n",
        "Vamos a entrenar el modelo con el conjunto de `train`, y probar con el conjunto de `devel` para encontrar el mejor N que maximice la performance del modelo. Asumimos que hay frases en devel que no estan en train, por lo tanto, el modelo no las conoce. Y a su vez, hay frases que son muy similares a las conocidas por el modelo. \n",
        "\n",
        "Para seleccionar el mejor modelo, vamos a tomar como frase 'bien' predicha aquella que tenga al menos una palabra que pertenezca a la frase original.\n",
        "\n",
        "Para evitar reducir el tiempo de ejecución, vamos a tomar un muestreo estadístico en devel, y vamos a tomar el intervalo de confianza del 95% para seleccionar el mejor modelo.\n",
        "\n",
        "****** revisar y completar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.bayesian_learning import G02NaiveBayesClassifier\n",
        "import os, numpy as np, pandas as pd\n",
        "import pdb \n",
        "\n",
        "meta_params_grid = {\n",
        "    'N': [1,2,3,4]\n",
        "}\n",
        "_devel = preprocessor.apply(devel) \n",
        "ITERATIONS = 5\n",
        "Z = 1.96\n",
        "P = 0.5\n",
        "E = 0.05\n",
        "_N = int(Z**2 * P * (1-P) / E**2)\n",
        "\n",
        "def grid_search_to_csv():\n",
        "    results = pd.DataFrame(columns=['N', 'iteration', 'lower_bound', 'upper_bound'])\n",
        "    for N in meta_params_grid['N']:\n",
        "        clf = G02NaiveBayesClassifier(train, N=N)\n",
        "        for i in range(ITERATIONS):\n",
        "            success = 0\n",
        "            unpredicted_sentences = []\n",
        "            for sentence in random.sample(_devel, _N): \n",
        "                aux = sentence[0:len(sentence)-1]\n",
        "                suggested_word = clf.predict(aux)\n",
        "                if (sentence[-1] == suggested_word): \n",
        "                    success+=1\n",
        "                else:\n",
        "                    unpredicted_sentences.append(' '.join(sentence))        \n",
        "                \n",
        "            for s in unpredicted_sentences:\n",
        "                clf.update(s)\n",
        "\n",
        "            _P = success/_N\n",
        "            delta = Z*np.sqrt(_P*(1-_P)/_N)\n",
        "            results.loc[len(results)] = [N, i, _P-delta, _P+delta]\n",
        "\n",
        "            print(f'N: {N} - Iteration: {i} - Success: {_P-delta, _P+delta}')\n",
        "    results.to_csv('assets/grid_search.csv', index=False)         \n",
        "\n",
        "if not os.path.exists('assets/grid_search.csv'): grid_search_to_csv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver graficamente como se comporta el modelo con distintos N."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "grid_search_csv= 'assets/grid_search.csv'\n",
        "\n",
        "results = pd.read_csv(grid_search_csv)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for n in results['N'].unique():\n",
        "    subset = results[results['N'] == n]\n",
        "    iterations = subset['iteration']\n",
        "    lower_bounds = subset['lower_bound']\n",
        "    upper_bounds = subset['upper_bound']\n",
        "    mean_values = [(lower + upper) / 2 for lower, upper in zip(lower_bounds, upper_bounds)]\n",
        "    \n",
        "    ax.plot(iterations, mean_values, label=f'N={n}')  \n",
        "    ax.fill_between(iterations, lower_bounds, upper_bounds, alpha=0.2)  \n",
        "\n",
        "\n",
        "ax.set_xlabel('Iteración')\n",
        "ax.set_ylabel('Success')\n",
        "ax.set_title('Evolución del Success por Iteración para diferentes valores de N')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimentación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulador de cliente.\n",
        "\n",
        "El experimento consiste en evaluar los distintos hiperparametros del modelo.\n",
        "\n",
        "El siguiente script permite simular el comportamiento de un cliente que escribe frases, fue proporcionado por el cuerpo docente. Es de utilidad para experimentar con modelo, ya que permite ingresar frases y ver las sugerencias que el modelo realiza.\n",
        "\n",
        "**Nota:**\n",
        "\n",
        "El comportamiento del siguiente script en `VSCode` es diferente al comportamiento en la terminal. En VSCode, el script omite entradas, se desfasa y hace que el analisis de los resultados sea dificil. Por lo tanto, recomendamos ejecutar el script en la terminal. (`client.py`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una breve conclusión del trabajo realizado. Por ejemplo: \n",
        "- ¿cuándo se dieron los mejores resultados del jugador?\n",
        "- ¿encuentra alguna relación con los parámetros / oponentes/ atributos elegidos?\n",
        "- ¿cómo mejoraría los resultados?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
